{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1867709e",
   "metadata": {},
   "source": [
    "# I M P L I C I T - Q - L E A R N I N G - IQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d3d7f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.set_float32_matmul_precision('highest')\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "\n",
    "import h5py\n",
    "import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6826e59c",
   "metadata": {},
   "source": [
    "### D E V I C E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03040398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7e22b3",
   "metadata": {},
   "source": [
    "### D A T A "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4a716e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of data: 1000000\n",
      "state dim: 11 | action dim: 3 | max action: 0.9999945163726807\n"
     ]
    }
   ],
   "source": [
    "file = 'C:\\OFFLINE RL\\hopper_medium-v2.hdf5'\n",
    "\n",
    "with h5py.File(file, mode = 'r') as f:\n",
    "    \n",
    "    observations = np.array(f['observations'])\n",
    "    actions = np.array(f['actions'])\n",
    "    rewards = np.array(f['rewards'])\n",
    "    next_obs = np.array(f['next_observations'])\n",
    "    \n",
    "state_dim = observations.shape[1]\n",
    "action_dim = actions.shape[1]\n",
    "max_action = abs(actions).max()\n",
    "\n",
    "print(f'len of data: {len(observations)}')\n",
    "print(f'state dim: {state_dim} | action dim: {action_dim} | max action: {max_action}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6cb095",
   "metadata": {},
   "source": [
    "### D A T A - E N G I N E E R I N G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5d8655b",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_tensor = torch.from_numpy(observations).float().to(device)\n",
    "act_tensor = torch.from_numpy(actions).float().to(device)\n",
    "rew_tensor = torch.from_numpy(rewards).float().to(device)\n",
    "next_obs_tensor = torch.from_numpy(next_obs).float().to(device)\n",
    "\n",
    "class Hopper_Dataset(Dataset):\n",
    "    \n",
    "    def __init__(self, obs, act, rew, next_obs_t):\n",
    "        \n",
    "        self.states = obs\n",
    "        self.actions = act\n",
    "        self.rewards = rew\n",
    "        self.next_states = next_obs_t\n",
    "        \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.states)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.states[index], self.actions[index], self.rewards[index], self.next_states[index]\n",
    "    \n",
    "dataset = Hopper_Dataset(obs_tensor, act_tensor, rew_tensor, next_obs_tensor)\n",
    "\n",
    "train_data, test_data = train_test_split(dataset, test_size = 0.1, shuffle = False)\n",
    "\n",
    "train_loader, test_loader = DataLoader(train_data, batch_size = 256, shuffle = True, drop_last = True), DataLoader(test_data, batch_size = 256, shuffle = True, drop_last = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bae6161",
   "metadata": {},
   "source": [
    "### L O G G I N G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3c14ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(log_dir = './runs/IQL')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1d9cbd",
   "metadata": {},
   "source": [
    "### A S S E M B L Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3f9acaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "head_1 = 128\n",
    "head_2 = 256\n",
    "head_3 = 256\n",
    "head_4 = 128\n",
    "\n",
    "hidden_size = 128\n",
    "hidden_size_2 = 256\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f2fcfa",
   "metadata": {},
   "source": [
    "### F E A T U R E "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d73b10d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feature_Extractor(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, hidden_size = hidden_size, hidden_size_2 = hidden_size_2):\n",
    "        super(Feature_Extractor, self).__init__()\n",
    "        \n",
    "        self.cal = nn.Sequential(\n",
    "            \n",
    "            nn.Linear(input_dim, hidden_size),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.Linear(hidden_size, hidden_size_2),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.Linear(hidden_size_2, hidden_size),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.Linear(hidden_size, output_dim),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "    \n",
    "        return self.cal(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d291e6",
   "metadata": {},
   "source": [
    "### P O L I C Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17440f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class policy_net(nn.Module):\n",
    "    \n",
    "    def __init__(self, head_1 = head_1, head_2 = head_2, head_3 = head_3, head_4 = head_4, max_action = max_action, state_dim = state_dim, action_dim = action_dim):\n",
    "        super(policy_net, self).__init__()\n",
    "        \n",
    "        # feature \n",
    "        \n",
    "        self.feature = Feature_Extractor(state_dim, head_1)\n",
    "        \n",
    "        # norm\n",
    "        \n",
    "        self.norm  = nn.LayerNorm(head_1)\n",
    "        \n",
    "        # pos process\n",
    "        \n",
    "        self.pos_process = nn.Sequential(\n",
    "            \n",
    "            nn.Linear(head_1, head_2),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.Linear(head_2, head_3),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.Linear(head_3, head_4),\n",
    "            nn.SiLU()\n",
    "        ) \n",
    "        \n",
    "        # mu and log std head\n",
    "        \n",
    "        self.mu = nn.Linear(head_4, action_dim)\n",
    "        self.log_std = nn.Linear(head_4, action_dim)\n",
    "        \n",
    "        # max action\n",
    "        \n",
    "        self.max_action = max_action\n",
    "        \n",
    "    def forward(self, state, deterministic = False, return_log_probs = False):\n",
    "        \n",
    "        # feature\n",
    "        \n",
    "        feature = self.feature(state)\n",
    "        \n",
    "        # norm\n",
    "        \n",
    "        norm = self.norm(feature)\n",
    "        \n",
    "        # pos\n",
    "        \n",
    "        pos = self.pos_process(norm)\n",
    "        \n",
    "        # mu and log std\n",
    "        \n",
    "        mu = self.mu(pos)\n",
    "        log_std = self.log_std(pos)\n",
    "        \n",
    "        log_std = torch.clamp(log_std, -10, 2)\n",
    "        std = torch.exp(log_std)\n",
    "        \n",
    "        dist = torch.distributions.Normal(mu, std)\n",
    "        z = mu if deterministic == True else dist.rsample()\n",
    "        tanh_z = torch.tanh(z)\n",
    "        action = tanh_z * self.max_action\n",
    "        \n",
    "        log_prob = None\n",
    "        \n",
    "        if return_log_probs:\n",
    "            \n",
    "            log_prob = dist.log_prob(z).sum(dim = -1, keepdim = True)\n",
    "            log_prob -= torch.log(1 - tanh_z.pow(2) + 1e-6).sum(dim = -1, keepdim = True)\n",
    "        \n",
    "        return action, mu, log_std, log_prob\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e055e818",
   "metadata": {},
   "source": [
    "### V A L U E "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23cf737a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class value_net(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_dim = state_dim, head_1 = head_1, head_2 = head_2, head_3 = head_3, head_4 = head_4):\n",
    "        super(value_net, self).__init__()\n",
    "        \n",
    "        # process\n",
    "        \n",
    "        self.process = nn.Sequential(\n",
    "            \n",
    "            nn.Linear(state_dim, head_1),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.Linear(head_1, head_2),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            \n",
    "            nn.LayerNorm(head_2),\n",
    "            nn.Linear(head_2, head_3),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            \n",
    "            nn.LayerNorm(head_3),\n",
    "            nn.Linear(head_3, head_4),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.Linear(head_4, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, state):\n",
    "        \n",
    "        value = self.process(state)\n",
    "        \n",
    "        return value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3000d964",
   "metadata": {},
   "source": [
    "### Q - F U N C T I O N "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3a2b9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class q_net(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_dim = state_dim, action_dim = action_dim, head_1 = head_1, head_2 = head_2, head_3 = head_3, head_4 = head_4):\n",
    "        super(q_net, self).__init__()\n",
    "        \n",
    "        self.q_1 = nn.Sequential(\n",
    "            \n",
    "            nn.Linear(state_dim + action_dim, head_1),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.Linear(head_1, head_2),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.LayerNorm(head_2),\n",
    "            nn.Linear(head_2, head_3),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.LayerNorm(head_3),\n",
    "            nn.Linear(head_3, head_4),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.Linear(head_4, 1)\n",
    "        )\n",
    "        \n",
    "        self.q_2 = nn.Sequential(\n",
    "            \n",
    "            nn.Linear(state_dim + action_dim, head_1),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.Linear(head_1, head_2),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.LayerNorm(head_2),\n",
    "            nn.Linear(head_2, head_3),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.LayerNorm(head_3),\n",
    "            nn.Linear(head_3, head_4),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.Linear(head_4, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        \n",
    "        # cat\n",
    "        \n",
    "        cat = torch.cat([state, action], dim = -1)\n",
    "        \n",
    "        # prepare\n",
    "        \n",
    "        q_1 = self.q_1(cat)\n",
    "        q_2 = self.q_2(cat)\n",
    "        \n",
    "        return q_1, q_2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a2f454",
   "metadata": {},
   "source": [
    "### S E T U P "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e22db9d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy_net(\n",
      "  (feature): Feature_Extractor(\n",
      "    (cal): Sequential(\n",
      "      (0): Linear(in_features=11, out_features=128, bias=True)\n",
      "      (1): SiLU()\n",
      "      (2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (3): Linear(in_features=128, out_features=256, bias=True)\n",
      "      (4): SiLU()\n",
      "      (5): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (6): SiLU()\n",
      "      (7): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (8): SiLU()\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (pos_process): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (1): SiLU()\n",
      "    (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (3): SiLU()\n",
      "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (5): SiLU()\n",
      "  )\n",
      "  (mu): Linear(in_features=128, out_features=3, bias=True)\n",
      "  (log_std): Linear(in_features=128, out_features=3, bias=True)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "value_net(\n",
      "  (process): Sequential(\n",
      "    (0): Linear(in_features=11, out_features=128, bias=True)\n",
      "    (1): SiLU()\n",
      "    (2): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (3): SiLU()\n",
      "    (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (5): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (6): SiLU()\n",
      "    (7): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (8): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (9): SiLU()\n",
      "    (10): Linear(in_features=128, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "q_net(\n",
      "  (q_1): Sequential(\n",
      "    (0): Linear(in_features=14, out_features=128, bias=True)\n",
      "    (1): SiLU()\n",
      "    (2): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (3): SiLU()\n",
      "    (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (5): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (6): SiLU()\n",
      "    (7): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (8): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (9): SiLU()\n",
      "    (10): Linear(in_features=128, out_features=1, bias=True)\n",
      "  )\n",
      "  (q_2): Sequential(\n",
      "    (0): Linear(in_features=14, out_features=128, bias=True)\n",
      "    (1): SiLU()\n",
      "    (2): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (3): SiLU()\n",
      "    (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (5): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (6): SiLU()\n",
      "    (7): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (8): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (9): SiLU()\n",
      "    (10): Linear(in_features=128, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "POLICY_NETWORK = policy_net().to(device)\n",
    "print(POLICY_NETWORK)\n",
    "print('-' * 100)\n",
    "\n",
    "VALUE_NETWORK = value_net().to(device)\n",
    "print(VALUE_NETWORK)\n",
    "print('-' * 100)\n",
    "\n",
    "Q_NETWORK = q_net().to(device)\n",
    "print(Q_NETWORK)\n",
    "print('-' * 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e58fa0",
   "metadata": {},
   "source": [
    "### O P T I M I Z E R - S C H E D U L E R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ac71d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr\n",
    "\n",
    "T_max = 100\n",
    "policy_lr = 1e-4\n",
    "value_lr = 1e-3\n",
    "q_lr = 1e-3\n",
    "\n",
    "# O P T I M I Z E R\n",
    "\n",
    "def get_optimizer(network, lr):\n",
    "    \n",
    "    return optim.AdamW(network.parameters(), lr, weight_decay = 0)\n",
    "\n",
    "policy_optimizer = get_optimizer(POLICY_NETWORK, policy_lr)\n",
    "value_optimizer = get_optimizer(VALUE_NETWORK, value_lr)\n",
    "q_net_optimizer = get_optimizer(Q_NETWORK, q_lr)\n",
    "\n",
    "# S C H E D U L E R\n",
    "\n",
    "def get_scheduler(optimizer, T_max = T_max):\n",
    "    \n",
    "    return optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max, eta_min = 1e-5)\n",
    "\n",
    "policy_scheduler = get_scheduler(policy_optimizer)\n",
    "value_scheduler = get_scheduler(value_optimizer)\n",
    "q_net_scheduler = get_scheduler(q_net_optimizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf915558",
   "metadata": {},
   "source": [
    "### L O S S - F U N C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8361e159",
   "metadata": {},
   "outputs": [],
   "source": [
    "class loss_func:\n",
    "    \n",
    "    def __init__(self, gamma, tau, beta, POLICY_NETWORK = POLICY_NETWORK, VALUE_NETWORK = VALUE_NETWORK, Q_NETWORK = Q_NETWORK, policy_optimizer = policy_optimizer, value_optimizer = value_optimizer, q_net_optimizer = q_net_optimizer, policy_scheduler = policy_scheduler, value_scheduler = value_scheduler, q_net_scheduler = q_net_scheduler):\n",
    "        \n",
    "        # network\n",
    "        \n",
    "        self.policy = POLICY_NETWORK\n",
    "        self.value = VALUE_NETWORK\n",
    "        self.q_network = Q_NETWORK\n",
    "        \n",
    "        # optimizer\n",
    "        \n",
    "        self.policy_optimizer = policy_optimizer\n",
    "        self.value_optimizer = value_optimizer\n",
    "        self.q_optimizer = q_net_optimizer\n",
    "        \n",
    "        # scheduler\n",
    "        \n",
    "        self.policy_scheduler = policy_scheduler\n",
    "        self.value_scheduler = value_scheduler\n",
    "        self.q_scheduler = q_net_scheduler\n",
    "        \n",
    "        # hyper params\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.beta = beta\n",
    "        \n",
    "    def value_loss(self, states, rewards, next_states):\n",
    "        \n",
    "        # current v\n",
    "        \n",
    "        v = self.value(states)\n",
    "        \n",
    "        # next v\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            next_v = self.value(next_states)\n",
    "            \n",
    "        delta = rewards + self.gamma * next_v - v\n",
    "        weight = torch.where(delta > 0, self.tau, 1 - self.tau)\n",
    "        \n",
    "        value_loss = (weight * delta.pow(2)).mean()\n",
    "        \n",
    "        return value_loss\n",
    "    \n",
    "    def q_loss(self, states, actions, rewards, next_states):\n",
    "        \n",
    "        # cal target value\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            next_v = self.value(next_states)\n",
    "            target = rewards + self.gamma * next_v\n",
    "            \n",
    "        q1, q2 = self.q_network(states, actions)\n",
    "        l1 = F.mse_loss(q1, target)\n",
    "        l2 = F.mse_loss(q2, target)\n",
    "        \n",
    "        loss = 0.5 * (l1 + l2)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def policy_loss(self, states):\n",
    "        \n",
    "        # cal log probs and actions\n",
    "        \n",
    "        actions, _, _, log_probs = self.policy(states, return_log_probs = True)\n",
    "        \n",
    "        # cal q vals\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            q1, q2 = self.q_network(states, actions)\n",
    "            \n",
    "            q = torch.min(q1, q2)\n",
    "            \n",
    "        # cal v\n",
    "        \n",
    "            v = self.value(states)\n",
    "            \n",
    "            advantages = q - v\n",
    "        \n",
    "        # cal weights\n",
    "        \n",
    "        weights = torch.exp(self.beta * advantages)\n",
    "        weights = torch.clamp(weights, max = 100.0)\n",
    "        \n",
    "        # cal policy loss\n",
    "        \n",
    "        policy_loss = -(weights * log_probs).mean()\n",
    "        \n",
    "        \n",
    "        return policy_loss, advantages\n",
    "    \n",
    "    def update(self, data_loader, epoch):\n",
    "        \n",
    "        running_policy_loss = 0.0\n",
    "        running_value_loss = 0.0\n",
    "        running_q_loss = 0.0\n",
    "        all_advantages = []\n",
    "        \n",
    "        \n",
    "        for states, actions, rewards, next_states in data_loader:\n",
    "            \n",
    "            if rewards.dim() == 1: rewards = rewards.unsqueeze(1)\n",
    "        \n",
    "            # cal value loss\n",
    "            \n",
    "            value_loss = self.value_loss(states, rewards, next_states)\n",
    "            running_value_loss += value_loss.item()\n",
    "            \n",
    "            # update value network\n",
    "            \n",
    "            self.value_optimizer.zero_grad()\n",
    "            value_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.value.parameters(), max_norm = 0.5)\n",
    "            self.value_optimizer.step()\n",
    "            self.value_scheduler.step()\n",
    "            \n",
    "            # cal q _ loss\n",
    "            \n",
    "            q_loss = self.q_loss(states, actions, rewards, next_states)\n",
    "            running_q_loss += q_loss.item()\n",
    "            \n",
    "            # update q network\n",
    "            \n",
    "            self.q_optimizer.zero_grad()\n",
    "            q_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), max_norm = 0.5)\n",
    "            self.q_optimizer.step()\n",
    "            self.q_scheduler.step()\n",
    "            \n",
    "            # policy loss\n",
    "            \n",
    "            policy_loss, advantages = self.policy_loss(states)\n",
    "            running_policy_loss += policy_loss.item()\n",
    "            all_advantages.append(advantages.detach().cpu())\n",
    "            \n",
    "            # update\n",
    "            \n",
    "            self.policy_optimizer.zero_grad()\n",
    "            policy_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.policy.parameters(), max_norm = 0.5)\n",
    "            self.policy_optimizer.step()\n",
    "            self.policy_scheduler.step()\n",
    "            \n",
    "        avg_value_loss = running_value_loss / len(data_loader)\n",
    "        avg_q_loss = running_q_loss / len(data_loader)\n",
    "        avg_policy_loss = running_policy_loss / len(data_loader)\n",
    "        \n",
    "        all_advantages = torch.cat(all_advantages, dim = 0)\n",
    "        writer.add_histogram('Advantages', all_advantages, epoch)\n",
    "        \n",
    "        return avg_value_loss, avg_q_loss, avg_policy_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c811810f",
   "metadata": {},
   "source": [
    "### S E T U P \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "06220f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper params\n",
    "\n",
    "gamma = 0.97\n",
    "tau = 0.7\n",
    "beta = 2.0\n",
    "\n",
    "# setup\n",
    "\n",
    "LOSS_FUNCTION = loss_func(gamma, tau, beta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f72e74",
   "metadata": {},
   "source": [
    "### T R A I N I N G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a144ad7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training IQL:   1%|          | 1/100 [01:28<2:26:50, 89.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 / 100 | avg v loss: 1.3859 | avg q loss: 84.1923 | avg policy loss: -948.2040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training IQL:   2%|▏         | 2/100 [02:33<2:02:00, 74.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 / 100 | avg v loss: 0.4220 | avg q loss: 26.8879 | avg policy loss: -1035.5075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training IQL:   3%|▎         | 3/100 [03:35<1:50:57, 68.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 / 100 | avg v loss: 0.3363 | avg q loss: 16.9098 | avg policy loss: -1007.7772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training IQL:   4%|▍         | 4/100 [04:35<1:44:36, 65.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 / 100 | avg v loss: 0.3244 | avg q loss: 16.3538 | avg policy loss: -983.1977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training IQL:   5%|▌         | 5/100 [05:54<1:51:10, 70.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 / 100 | avg v loss: 0.2972 | avg q loss: 15.5811 | avg policy loss: -957.3058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training IQL:   6%|▌         | 6/100 [07:23<1:59:56, 76.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 / 100 | avg v loss: 0.2784 | avg q loss: 13.8399 | avg policy loss: -913.3911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training IQL:   7%|▋         | 7/100 [08:50<2:04:18, 80.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 / 100 | avg v loss: 0.2698 | avg q loss: 13.1250 | avg policy loss: -878.2010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training IQL:   8%|▊         | 8/100 [10:16<2:05:32, 81.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 / 100 | avg v loss: 0.2401 | avg q loss: 10.4078 | avg policy loss: -809.1322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training IQL:   9%|▉         | 9/100 [11:44<2:07:07, 83.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 / 100 | avg v loss: 0.2385 | avg q loss: 12.4915 | avg policy loss: -789.0014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training IQL:  10%|█         | 10/100 [13:09<2:06:28, 84.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 / 100 | avg v loss: 0.2343 | avg q loss: 11.3595 | avg policy loss: -708.0381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training IQL:  11%|█         | 11/100 [14:33<2:04:50, 84.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 / 100 | avg v loss: 0.2415 | avg q loss: 12.4804 | avg policy loss: -728.6002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training IQL:  12%|█▏        | 12/100 [15:54<2:02:01, 83.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11 / 100 | avg v loss: 0.2294 | avg q loss: 10.7120 | avg policy loss: -656.9594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training IQL:  13%|█▎        | 13/100 [17:00<1:52:56, 77.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12 / 100 | avg v loss: 0.2333 | avg q loss: 10.2143 | avg policy loss: -616.1512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training IQL:  14%|█▍        | 14/100 [18:05<1:46:02, 73.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13 / 100 | avg v loss: 0.2202 | avg q loss: 10.1577 | avg policy loss: -641.1785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training IQL:  15%|█▌        | 15/100 [19:12<1:41:58, 71.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14 / 100 | avg v loss: 0.2134 | avg q loss: 10.7072 | avg policy loss: -646.0024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training IQL:  16%|█▌        | 16/100 [20:13<1:36:18, 68.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15 / 100 | avg v loss: 0.2113 | avg q loss: 10.4909 | avg policy loss: -615.0130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training IQL:  17%|█▋        | 17/100 [21:19<1:33:59, 67.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16 / 100 | avg v loss: 0.2111 | avg q loss: 9.4724 | avg policy loss: -602.8571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training IQL:  18%|█▊        | 18/100 [22:33<1:35:09, 69.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17 / 100 | avg v loss: 0.2077 | avg q loss: 8.5919 | avg policy loss: -586.5915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training IQL:  19%|█▉        | 19/100 [23:38<1:32:06, 68.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18 / 100 | avg v loss: 0.1979 | avg q loss: 8.5397 | avg policy loss: -590.9516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training IQL:  20%|██        | 20/100 [24:42<1:29:22, 67.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19 / 100 | avg v loss: 0.2024 | avg q loss: 9.3672 | avg policy loss: -586.6571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training IQL:  21%|██        | 21/100 [25:47<1:27:25, 66.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20 / 100 | avg v loss: 0.1929 | avg q loss: 8.1910 | avg policy loss: -603.4874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training IQL:  22%|██▏       | 22/100 [27:10<1:32:52, 71.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21 / 100 | avg v loss: 0.1794 | avg q loss: 7.5313 | avg policy loss: -572.0566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training IQL:  23%|██▎       | 23/100 [28:20<1:31:03, 70.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22 / 100 | avg v loss: 0.1812 | avg q loss: 7.6855 | avg policy loss: -581.7412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training IQL:  24%|██▍       | 24/100 [29:25<1:27:21, 68.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23 / 100 | avg v loss: 0.1802 | avg q loss: 7.5148 | avg policy loss: -574.3077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training IQL:  25%|██▌       | 25/100 [30:28<1:24:12, 67.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24 / 100 | avg v loss: 0.1715 | avg q loss: 7.5830 | avg policy loss: -555.4828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training IQL:  26%|██▌       | 26/100 [31:36<1:23:20, 67.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25 / 100 | avg v loss: 0.1726 | avg q loss: 6.4475 | avg policy loss: -584.0136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training IQL:  27%|██▋       | 27/100 [32:48<1:23:39, 68.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26 / 100 | avg v loss: 0.1651 | avg q loss: 7.4918 | avg policy loss: -590.4853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training IQL:  28%|██▊       | 28/100 [33:52<1:20:57, 67.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27 / 100 | avg v loss: 0.1544 | avg q loss: 6.6567 | avg policy loss: -537.9736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training IQL:  29%|██▉       | 29/100 [35:05<1:21:50, 69.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28 / 100 | avg v loss: 0.1614 | avg q loss: 6.0530 | avg policy loss: -571.1491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training IQL:  30%|███       | 30/100 [36:07<1:17:57, 66.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29 / 100 | avg v loss: 0.1665 | avg q loss: 7.1615 | avg policy loss: -585.1828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training IQL:  31%|███       | 31/100 [37:07<1:14:44, 64.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30 / 100 | avg v loss: 0.1900 | avg q loss: 7.3615 | avg policy loss: -587.9151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training IQL:  32%|███▏      | 32/100 [38:16<1:14:43, 65.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 31 / 100 | avg v loss: 0.1700 | avg q loss: 6.7824 | avg policy loss: -558.4485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training IQL:  33%|███▎      | 33/100 [39:23<1:14:05, 66.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 32 / 100 | avg v loss: 0.1698 | avg q loss: 6.6882 | avg policy loss: -582.4580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training IQL:  34%|███▍      | 34/100 [40:30<1:13:22, 66.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 33 / 100 | avg v loss: 0.1647 | avg q loss: 6.5308 | avg policy loss: -530.4409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training IQL:  35%|███▌      | 35/100 [41:38<1:12:37, 67.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 34 / 100 | avg v loss: 0.1686 | avg q loss: 5.9968 | avg policy loss: -526.2778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training IQL:  36%|███▌      | 36/100 [42:40<1:09:52, 65.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 35 / 100 | avg v loss: 0.1577 | avg q loss: 6.1140 | avg policy loss: -558.8939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training IQL:  37%|███▋      | 37/100 [43:40<1:07:08, 63.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 36 / 100 | avg v loss: 0.1507 | avg q loss: 6.0059 | avg policy loss: -567.6307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training IQL:  38%|███▊      | 38/100 [44:49<1:07:32, 65.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 37 / 100 | avg v loss: 0.1585 | avg q loss: 5.6388 | avg policy loss: -556.3631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training IQL:  39%|███▉      | 39/100 [45:53<1:06:06, 65.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 38 / 100 | avg v loss: 0.1586 | avg q loss: 5.5372 | avg policy loss: -554.9001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training IQL:  40%|████      | 40/100 [46:57<1:04:37, 64.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 39 / 100 | avg v loss: 0.1544 | avg q loss: 5.7016 | avg policy loss: -555.8178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training IQL:  41%|████      | 41/100 [48:03<1:03:49, 64.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 40 / 100 | avg v loss: 0.1520 | avg q loss: 5.8189 | avg policy loss: -538.4936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training IQL:  42%|████▏     | 42/100 [49:05<1:02:07, 64.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 41 / 100 | avg v loss: 0.1631 | avg q loss: 5.7670 | avg policy loss: -556.3157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training IQL:  43%|████▎     | 43/100 [50:12<1:01:44, 65.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 42 / 100 | avg v loss: 0.1514 | avg q loss: 5.6906 | avg policy loss: -567.3556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training IQL:  44%|████▍     | 44/100 [51:17<1:00:31, 64.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 43 / 100 | avg v loss: 0.1594 | avg q loss: 5.8813 | avg policy loss: -575.6659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training IQL:  45%|████▌     | 45/100 [52:22<59:40, 65.10s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 44 / 100 | avg v loss: 0.1422 | avg q loss: 5.0768 | avg policy loss: -547.9258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training IQL:  46%|████▌     | 46/100 [53:26<58:09, 64.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 45 / 100 | avg v loss: 0.1610 | avg q loss: 6.1169 | avg policy loss: -570.6060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training IQL:  47%|████▋     | 47/100 [54:25<55:40, 63.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 46 / 100 | avg v loss: 0.1523 | avg q loss: 5.8694 | avg policy loss: -549.3955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training IQL:  48%|████▊     | 48/100 [55:27<54:17, 62.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 47 / 100 | avg v loss: 0.1692 | avg q loss: 5.5244 | avg policy loss: -573.0074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training IQL:  49%|████▉     | 49/100 [56:38<55:22, 65.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 48 / 100 | avg v loss: 0.1540 | avg q loss: 5.5848 | avg policy loss: -557.9532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training IQL:  50%|█████     | 50/100 [57:58<58:00, 69.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 49 / 100 | avg v loss: 0.1480 | avg q loss: 4.9120 | avg policy loss: -546.7591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training IQL:  51%|█████     | 51/100 [59:03<55:39, 68.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 / 100 | avg v loss: 0.1536 | avg q loss: 5.2622 | avg policy loss: -542.1519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training IQL:  52%|█████▏    | 52/100 [1:00:05<53:05, 66.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 51 / 100 | avg v loss: 0.1477 | avg q loss: 4.8330 | avg policy loss: -561.9044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training IQL:  53%|█████▎    | 53/100 [1:01:09<51:33, 65.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 52 / 100 | avg v loss: 0.1474 | avg q loss: 4.9551 | avg policy loss: -564.5768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training IQL:  54%|█████▍    | 54/100 [1:02:15<50:20, 65.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 53 / 100 | avg v loss: 0.1594 | avg q loss: 4.6637 | avg policy loss: -546.2826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training IQL:  55%|█████▌    | 55/100 [1:03:28<51:02, 68.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 54 / 100 | avg v loss: 0.1375 | avg q loss: 4.3982 | avg policy loss: -545.7865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training IQL:  56%|█████▌    | 56/100 [1:04:37<50:01, 68.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 55 / 100 | avg v loss: 0.1474 | avg q loss: 4.7570 | avg policy loss: -544.5940\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training IQL:  57%|█████▋    | 57/100 [1:05:39<47:31, 66.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 56 / 100 | avg v loss: 0.1399 | avg q loss: 4.7195 | avg policy loss: -552.2599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training IQL:  58%|█████▊    | 58/100 [1:06:40<45:24, 64.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 57 / 100 | avg v loss: 0.1374 | avg q loss: 4.5671 | avg policy loss: -537.8728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training IQL:  59%|█████▉    | 59/100 [1:07:42<43:36, 63.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 58 / 100 | avg v loss: 0.1511 | avg q loss: 4.3374 | avg policy loss: -506.9408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training IQL:  60%|██████    | 60/100 [1:08:46<42:39, 63.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 59 / 100 | avg v loss: 0.1384 | avg q loss: 4.6709 | avg policy loss: -536.3736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training IQL:  61%|██████    | 61/100 [1:09:47<41:05, 63.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 60 / 100 | avg v loss: 0.1363 | avg q loss: 4.2096 | avg policy loss: -527.0669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training IQL:  62%|██████▏   | 62/100 [1:10:47<39:25, 62.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 61 / 100 | avg v loss: 0.1384 | avg q loss: 4.2015 | avg policy loss: -522.4280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training IQL:  63%|██████▎   | 63/100 [1:11:48<38:05, 61.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 62 / 100 | avg v loss: 0.1291 | avg q loss: 4.1989 | avg policy loss: -506.2030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training IQL:  64%|██████▍   | 64/100 [1:12:49<36:53, 61.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 63 / 100 | avg v loss: 0.1319 | avg q loss: 3.7546 | avg policy loss: -485.6794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training IQL:  65%|██████▌   | 65/100 [1:14:04<38:13, 65.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 64 / 100 | avg v loss: 0.1384 | avg q loss: 4.3040 | avg policy loss: -480.9979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training IQL:  66%|██████▌   | 66/100 [1:15:05<36:26, 64.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 65 / 100 | avg v loss: 0.1365 | avg q loss: 4.1112 | avg policy loss: -488.4565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training IQL:  67%|██████▋   | 67/100 [1:16:05<34:37, 62.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 66 / 100 | avg v loss: 0.1286 | avg q loss: 3.9397 | avg policy loss: -476.9576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training IQL:  68%|██████▊   | 68/100 [1:17:06<33:15, 62.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 67 / 100 | avg v loss: 0.1306 | avg q loss: 4.4507 | avg policy loss: -490.4714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training IQL:  69%|██████▉   | 69/100 [1:18:06<31:53, 61.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 68 / 100 | avg v loss: 0.1371 | avg q loss: 3.9386 | avg policy loss: -474.0506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training IQL:  70%|███████   | 70/100 [1:19:07<30:42, 61.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 69 / 100 | avg v loss: 0.1301 | avg q loss: 3.9215 | avg policy loss: -491.3894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training IQL:  71%|███████   | 71/100 [1:20:10<29:59, 62.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 70 / 100 | avg v loss: 0.1325 | avg q loss: 4.2452 | avg policy loss: -478.6582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training IQL:  72%|███████▏  | 72/100 [1:21:10<28:38, 61.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 71 / 100 | avg v loss: 0.1301 | avg q loss: 4.0651 | avg policy loss: -485.4012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training IQL:  73%|███████▎  | 73/100 [1:22:13<27:50, 61.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 72 / 100 | avg v loss: 0.1387 | avg q loss: 4.1550 | avg policy loss: -478.0781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training IQL:  74%|███████▍  | 74/100 [1:23:17<27:04, 62.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 73 / 100 | avg v loss: 0.1367 | avg q loss: 4.2156 | avg policy loss: -494.1085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training IQL:  75%|███████▌  | 75/100 [1:24:26<26:47, 64.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 74 / 100 | avg v loss: 0.1356 | avg q loss: 3.9420 | avg policy loss: -474.7700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training IQL:  76%|███████▌  | 76/100 [1:25:33<26:06, 65.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 75 / 100 | avg v loss: 0.1299 | avg q loss: 3.4255 | avg policy loss: -450.1687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training IQL:  77%|███████▋  | 77/100 [1:26:36<24:45, 64.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 76 / 100 | avg v loss: 0.1405 | avg q loss: 3.8855 | avg policy loss: -454.4845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training IQL:  78%|███████▊  | 78/100 [1:27:38<23:24, 63.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 77 / 100 | avg v loss: 0.1279 | avg q loss: 3.4315 | avg policy loss: -453.5228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training IQL:  79%|███████▉  | 79/100 [1:28:39<22:02, 62.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 78 / 100 | avg v loss: 0.1305 | avg q loss: 3.7105 | avg policy loss: -453.9908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training IQL:  80%|████████  | 80/100 [1:29:42<21:01, 63.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 79 / 100 | avg v loss: 0.1239 | avg q loss: 3.5652 | avg policy loss: -446.6634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training IQL:  81%|████████  | 81/100 [1:30:45<19:52, 62.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 80 / 100 | avg v loss: 0.1260 | avg q loss: 3.5769 | avg policy loss: -445.2010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training IQL:  82%|████████▏ | 82/100 [1:31:47<18:46, 62.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 81 / 100 | avg v loss: 0.1210 | avg q loss: 3.1479 | avg policy loss: -427.7220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training IQL:  83%|████████▎ | 83/100 [1:32:48<17:37, 62.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 82 / 100 | avg v loss: 0.1125 | avg q loss: 3.2369 | avg policy loss: -466.4597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training IQL:  84%|████████▍ | 84/100 [1:33:49<16:28, 61.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 83 / 100 | avg v loss: 0.1204 | avg q loss: 3.4668 | avg policy loss: -441.0920\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training IQL:  85%|████████▌ | 85/100 [1:34:51<15:26, 61.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 84 / 100 | avg v loss: 0.1250 | avg q loss: 3.5433 | avg policy loss: -437.3539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training IQL:  86%|████████▌ | 86/100 [1:35:52<14:24, 61.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 85 / 100 | avg v loss: 0.1215 | avg q loss: 2.9917 | avg policy loss: -426.5437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training IQL:  87%|████████▋ | 87/100 [1:36:55<13:26, 62.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 86 / 100 | avg v loss: 0.1156 | avg q loss: 3.8092 | avg policy loss: -450.3662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training IQL:  88%|████████▊ | 88/100 [1:37:57<12:23, 61.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 87 / 100 | avg v loss: 0.1198 | avg q loss: 3.4020 | avg policy loss: -437.3512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training IQL:  89%|████████▉ | 89/100 [1:38:59<11:21, 61.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 88 / 100 | avg v loss: 0.1191 | avg q loss: 3.2987 | avg policy loss: -459.9882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training IQL:  90%|█████████ | 90/100 [1:40:01<10:19, 61.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 89 / 100 | avg v loss: 0.1129 | avg q loss: 2.8400 | avg policy loss: -412.0021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training IQL:  91%|█████████ | 91/100 [1:41:10<09:36, 64.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 90 / 100 | avg v loss: 0.1259 | avg q loss: 3.3485 | avg policy loss: -439.5145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training IQL:  92%|█████████▏| 92/100 [1:42:17<08:40, 65.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 91 / 100 | avg v loss: 0.1081 | avg q loss: 2.8144 | avg policy loss: -419.3894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training IQL:  93%|█████████▎| 93/100 [1:43:36<08:05, 69.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 92 / 100 | avg v loss: 0.1113 | avg q loss: 3.2158 | avg policy loss: -432.3067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training IQL:  94%|█████████▍| 94/100 [1:44:50<07:03, 70.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 93 / 100 | avg v loss: 0.1066 | avg q loss: 2.9034 | avg policy loss: -410.2417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training IQL:  95%|█████████▌| 95/100 [1:46:00<05:52, 70.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 94 / 100 | avg v loss: 0.1197 | avg q loss: 3.4898 | avg policy loss: -417.3898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training IQL:  96%|█████████▌| 96/100 [1:47:12<04:44, 71.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 95 / 100 | avg v loss: 0.1099 | avg q loss: 3.2162 | avg policy loss: -422.7069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training IQL:  97%|█████████▋| 97/100 [1:48:23<03:32, 70.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 96 / 100 | avg v loss: 0.1138 | avg q loss: 3.2863 | avg policy loss: -433.9003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training IQL:  98%|█████████▊| 98/100 [1:49:32<02:20, 70.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 97 / 100 | avg v loss: 0.1120 | avg q loss: 2.9351 | avg policy loss: -397.9808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training IQL:  99%|█████████▉| 99/100 [1:50:42<01:10, 70.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 98 / 100 | avg v loss: 0.1137 | avg q loss: 2.8402 | avg policy loss: -412.6640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training IQL: 100%|██████████| 100/100 [1:52:11<00:00, 67.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 99 / 100 | avg v loss: 0.1150 | avg q loss: 2.9650 | avg policy loss: -380.7476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "\n",
    "for epoch in tqdm.tqdm(range(epochs), desc = 'Training IQL'):\n",
    "    \n",
    "    avg_value_loss, avg_q_loss, avg_policy_loss = LOSS_FUNCTION.update(train_loader, epoch)\n",
    "    \n",
    "    writer.add_scalar('Value loss', avg_value_loss, epoch)\n",
    "    writer.add_scalar('Q Loss', avg_q_loss, epoch)\n",
    "    writer.add_scalar('Policy loss', avg_policy_loss, epoch)\n",
    "    \n",
    "    writer.flush()\n",
    "    \n",
    "    tqdm.tqdm.write(f'Epoch: {epoch} / {epochs} | '\n",
    "                    f'avg v loss: {avg_value_loss:.4f} | '\n",
    "                    f'avg q loss: {avg_q_loss:.4f} | '\n",
    "                    f'avg policy loss: {avg_policy_loss:.4f}')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
