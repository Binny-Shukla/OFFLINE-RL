{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "420396fd",
   "metadata": {},
   "source": [
    "# **T - T** *(Trajectory Transformer)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54683b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import h5py\n",
    "import math\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5061efa2",
   "metadata": {},
   "source": [
    "### **DEVICE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6913d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f909f82",
   "metadata": {},
   "source": [
    "### LOGGING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d867df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(log_dir = './runs/TT')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd80f978",
   "metadata": {},
   "source": [
    "### **HYPER PARAMS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a7a4dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "head_1 = 128\n",
    "head_2 = 256\n",
    "head_3 = 256\n",
    "head_4 = 128\n",
    "\n",
    "K = 64\n",
    "batch_size = 128\n",
    "num_heads = 2\n",
    "\n",
    "reward_dim = 1\n",
    "\n",
    "maxlen = 10_000\n",
    "dropout = 0.1\n",
    "num_layers = 8\n",
    "\n",
    "state_vocab = 512\n",
    "action_vocab = 128\n",
    "reward_vocab = 16\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ad8538",
   "metadata": {},
   "source": [
    "### **DATA ENGINEERING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "360f81c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state dim: 11 | action dim: 3 | max action: 0.9999945163726807 | dataset size: 1000000\n"
     ]
    }
   ],
   "source": [
    "path = r\"C:\\OFFLINE RL\\hopper_medium-v2.hdf5\"\n",
    "\n",
    "with h5py.File(path, mode = 'r') as f:\n",
    "    \n",
    "    obs = np.array(f['observations'])\n",
    "    act = np.array(f['actions'])\n",
    "    rew = np.array(f['rewards'])\n",
    "    next_obs = np.array(f['next_observations'])\n",
    "    terminals = np.array(f['terminals'])\n",
    "    timeouts = np.array(f['timeouts'])\n",
    "    \n",
    "done = timeouts | terminals\n",
    "\n",
    "state_dim = obs.shape[1]\n",
    "action_dim = act.shape[1]\n",
    "max_action = np.abs(act).max()\n",
    "dataset_size = obs.shape[0]\n",
    "\n",
    "print(f'state dim: {state_dim} | action dim: {action_dim} | max action: {max_action} | dataset size: {dataset_size}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f721f3f",
   "metadata": {},
   "source": [
    "### **DATA HANDLING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3eb0c25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sequence\n",
    "\n",
    "episodes = []\n",
    "start = 0\n",
    "\n",
    "for i in range(len(obs)):\n",
    "    \n",
    "    if done[i] == 1:\n",
    "        \n",
    "        episodes.append((obs[start: i + 1], act[start: i + 1], rew[start: i + 1], next_obs[start: i + 1]))\n",
    "        \n",
    "if start > len(obs):\n",
    "    \n",
    "    episodes.append((obs[start: ], act[start: ], rew[start: ], next_obs[start: ]))\n",
    "\n",
    "class HopperDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, episodes, K):\n",
    "        \n",
    "        self.episodes = episodes\n",
    "        self.K = K\n",
    "        \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.episodes)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        obs, act, rew, next_obs = self.episodes[idx]\n",
    "        \n",
    "        t_len = len(obs)\n",
    "        \n",
    "        if t_len >= self.K:\n",
    "            \n",
    "            start = random.randint(0, t_len - self.K)\n",
    "                        \n",
    "            obs = obs[start: start + self.K]\n",
    "            act = act[start: start + self.K]\n",
    "            rew = rew[start: start + self.K]\n",
    "            next_obs = next_obs[start: start + self.K]\n",
    "            \n",
    "        else: # padding\n",
    "            \n",
    "            pad_len = self.K - t_len\n",
    "            \n",
    "            obs = np.concatenate([obs, np.zeros((pad_len, obs.shape[1]))], axis = 0)\n",
    "            act = np.concatenate([act, np.zeros((pad_len, act.shape[1]))], axis = 0)\n",
    "            rew = np.concatenate([rew, np.zeros(pad_len)], axis = 0)\n",
    "            next_obs = np.concatenate([next_obs, np.zeros((pad_len, next_obs.shape[1]))], axis = 0)\n",
    "            \n",
    "        return (\n",
    "            \n",
    "            torch.from_numpy(obs).float().to(device),\n",
    "            torch.from_numpy(act).float().to(device),\n",
    "            torch.from_numpy(rew).float().to(device).unsqueeze(-1),\n",
    "            torch.from_numpy(next_obs).float().to(device)\n",
    "            \n",
    "        )\n",
    "    \n",
    "\n",
    "hopper_dataset = HopperDataset(episodes, K)\n",
    "\n",
    "data_loader = DataLoader(hopper_dataset, batch_size, shuffle = True, drop_last = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e2909f",
   "metadata": {},
   "source": [
    "### **INPUT EMBEDDINGS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a742c002",
   "metadata": {},
   "outputs": [],
   "source": [
    "class input_embedding(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_dim = state_dim, action_dim = action_dim, reward_dim = reward_dim, head_1 = head_1, head_2 = head_2, head_3 = head_3, head_4 = head_4):\n",
    "        super(input_embedding, self).__init__()\n",
    "        \n",
    "        def create_embed(input_dim):\n",
    "            \n",
    "            embed = nn.Sequential(\n",
    "                \n",
    "                nn.Linear(input_dim, head_1),\n",
    "                nn.LayerNorm(head_1),\n",
    "                nn.SiLU(),\n",
    "                \n",
    "                nn.Linear(head_1, head_2),\n",
    "                nn.LayerNorm(head_2),\n",
    "                nn.SiLU(),\n",
    "                \n",
    "                nn.Linear(head_2, head_3),\n",
    "                nn.LayerNorm(head_3),\n",
    "                nn.SiLU(),\n",
    "                \n",
    "                nn.Linear(head_3, head_4)\n",
    "                \n",
    "            )\n",
    "            \n",
    "            return embed\n",
    "        \n",
    "        # create embeds \n",
    "        \n",
    "        self.state_embed = create_embed(state_dim)\n",
    "        self.action_embed = create_embed(action_dim)\n",
    "        self.reward_embed = create_embed(reward_dim)\n",
    "        \n",
    "    def forward(self, state, action, reward):\n",
    "        \n",
    "        state_embed = self.state_embed(state)\n",
    "        action_embed = self.action_embed(action)\n",
    "        reward_embed = self.reward_embed(reward)\n",
    "        \n",
    "        return state_embed, action_embed, reward_embed\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2db20bf",
   "metadata": {},
   "source": [
    "### **SETUP**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ad6b758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_embedding(\n",
      "  (state_embed): Sequential(\n",
      "    (0): Linear(in_features=11, out_features=128, bias=True)\n",
      "    (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (2): SiLU()\n",
      "    (3): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (5): SiLU()\n",
      "    (6): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (7): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (8): SiLU()\n",
      "    (9): Linear(in_features=256, out_features=128, bias=True)\n",
      "  )\n",
      "  (action_embed): Sequential(\n",
      "    (0): Linear(in_features=3, out_features=128, bias=True)\n",
      "    (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (2): SiLU()\n",
      "    (3): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (5): SiLU()\n",
      "    (6): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (7): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (8): SiLU()\n",
      "    (9): Linear(in_features=256, out_features=128, bias=True)\n",
      "  )\n",
      "  (reward_embed): Sequential(\n",
      "    (0): Linear(in_features=1, out_features=128, bias=True)\n",
      "    (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (2): SiLU()\n",
      "    (3): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (5): SiLU()\n",
      "    (6): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (7): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (8): SiLU()\n",
      "    (9): Linear(in_features=256, out_features=128, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "INPUT_EMBEDDINGS = input_embedding().to(device)\n",
    "print(INPUT_EMBEDDINGS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24e2757",
   "metadata": {},
   "source": [
    "### **POSITIONAL ENCODING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc4d4fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class pos_encoding(nn.Module):\n",
    "    \n",
    "    def __init__(self, maxlen = maxlen, head_4 = head_4):\n",
    "        super().__init__()\n",
    "        \n",
    "        pe = torch.zeros(maxlen, head_4)\n",
    "        pos = torch.arange(0, maxlen, dtype = torch.float32).unsqueeze(1)\n",
    "        \n",
    "        div_term = torch.exp(torch.arange(0, head_4, 2).float() * (-math.log(10_000.0) / head_4))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(pos * div_term)\n",
    "        pe[:, 1::2] = torch.cos(pos * div_term)\n",
    "        \n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        seq_length = x.size(1)\n",
    "        \n",
    "        x = x + self.pe[:, :seq_length]\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e1407e",
   "metadata": {},
   "source": [
    "### **SETUP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5353facc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_encoding()\n"
     ]
    }
   ],
   "source": [
    "POSITIONAL_ENCODING = pos_encoding()\n",
    "\n",
    "print(POSITIONAL_ENCODING)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8add8252",
   "metadata": {},
   "source": [
    "### **MASKING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89fbae32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def casual_mask(seq_length, device):\n",
    "    \n",
    "    mask = torch.tril(torch.ones((seq_length, seq_length), device = device))\n",
    "    \n",
    "    return mask.unsqueeze(0).unsqueeze(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4c3e89",
   "metadata": {},
   "source": [
    "### **MULTI HEAD ATTENTION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f7efc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multi_Head_Attention(nn.Module):\n",
    "    \n",
    "    def __init__(self, head_4 = head_4, num_heads = num_heads):\n",
    "        super(Multi_Head_Attention, self).__init__()\n",
    "        \n",
    "        assert head_4 % num_heads == 0\n",
    "        \n",
    "        self.head_dim = head_4 // num_heads\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        self.Q = nn.Linear(head_4, head_4)\n",
    "        self.V = nn.Linear(head_4, head_4)\n",
    "        self.K = nn.Linear(head_4, head_4)\n",
    "        \n",
    "        self.norm = nn.LayerNorm(head_4)\n",
    "        self.proj = nn.Linear(head_4, head_4)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        B, T, D = x.size()\n",
    "        \n",
    "        Q = self.Q(x)\n",
    "        K = self.K(x)\n",
    "        V = self.V(x)\n",
    "        \n",
    "        Q = Q.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = K.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = V.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        \n",
    "        mask = casual_mask(T, Q.device)\n",
    "        \n",
    "        scores = scores.masked_fill(mask == 0, -np.inf)\n",
    "        \n",
    "        weights = torch.softmax(scores, dim = -1)\n",
    "        \n",
    "        attn = torch.matmul(weights, V)\n",
    "        \n",
    "        attn = attn.transpose(1, 2).contiguous().view(B, T, D)\n",
    "        \n",
    "        out = self.norm(self.proj(attn))\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623472b4",
   "metadata": {},
   "source": [
    "### **SET UP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c58d8b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi_Head_Attention(\n",
      "  (Q): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (V): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (K): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (proj): Linear(in_features=128, out_features=128, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "MHA = Multi_Head_Attention()\n",
    "\n",
    "print(MHA)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89674631",
   "metadata": {},
   "source": [
    "### **FEED FORWARD NETWORK**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26e877db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class feed_forward_network(nn.Module):\n",
    "    \n",
    "    def __init__(self, head_4 = head_4, head_3 = head_3):\n",
    "        super(feed_forward_network, self).__init__()\n",
    "        \n",
    "        self.pre_norm = nn.LayerNorm(head_4)\n",
    "        \n",
    "        self.ffn = nn.Sequential(\n",
    "            \n",
    "            nn.Linear(head_4, head_3),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.Linear(head_3, head_4)\n",
    "            \n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        \n",
    "        norm = self.pre_norm(x)\n",
    "        \n",
    "        ffn = self.ffn(norm)\n",
    "        \n",
    "        x = x + ffn\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bffb89",
   "metadata": {},
   "source": [
    "### **SETUP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a2a40fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feed_forward_network(\n",
      "  (pre_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (ffn): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (1): SiLU()\n",
      "    (2): Linear(in_features=256, out_features=128, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "FEED_FORWARD_NETWORK = feed_forward_network()\n",
    "\n",
    "print(FEED_FORWARD_NETWORK)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ea62e5",
   "metadata": {},
   "source": [
    "### **TRANSFORMER BLOCK**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e352b821",
   "metadata": {},
   "outputs": [],
   "source": [
    "class transformer_block(nn.Module):\n",
    "    \n",
    "    def __init__(self, dropout = dropout, head_4 = head_4):\n",
    "        super(transformer_block, self).__init__()\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(head_4)\n",
    "        self.mha = Multi_Head_Attention()\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        \n",
    "        self.norm2 = nn.LayerNorm(head_4)\n",
    "        self.ffn = feed_forward_network()\n",
    "        self.drop2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        norm1 = self.norm1(x)\n",
    "        attn = self.mha(norm1)\n",
    "        x = x + self.drop(attn)\n",
    "        \n",
    "        norm2 = self.norm2(x)\n",
    "        ffn = self.ffn(norm2)\n",
    "        x = x + self.drop2(ffn)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b62a99",
   "metadata": {},
   "source": [
    "### **SETUP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b387f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer_block(\n",
      "  (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (mha): Multi_Head_Attention(\n",
      "    (Q): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (V): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (K): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "  )\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      "  (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (ffn): feed_forward_network(\n",
      "    (pre_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (ffn): Sequential(\n",
      "      (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "      (1): SiLU()\n",
      "      (2): Linear(in_features=256, out_features=128, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (drop2): Dropout(p=0.1, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "TRANSFORMER_BLOCK = transformer_block()\n",
    "\n",
    "print(TRANSFORMER_BLOCK)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734e7d0d",
   "metadata": {},
   "source": [
    "### **PREDICTION HEAD**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e104ebe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class prediction_net(nn.Module):\n",
    "    \n",
    "    def __init__(self, head_4 = head_4, state_dim = state_dim, action_dim = action_dim, reward_dim = reward_dim):\n",
    "        super(prediction_net, self).__init__()\n",
    "        \n",
    "        \n",
    "        def create(output_dim):\n",
    "        \n",
    "            mlp = nn.Sequential(\n",
    "                \n",
    "                nn.Linear(head_4, head_4),\n",
    "                nn.SiLU(),\n",
    "                nn.Linear(head_4, output_dim)\n",
    "                \n",
    "            )\n",
    "            \n",
    "            return mlp\n",
    "        \n",
    "        self.pred_state = create(state_dim)\n",
    "        self.pred_action = create(action_dim)\n",
    "        self.pred_reward = create(reward_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        pred_state = self.pred_state(x)\n",
    "        pred_action = self.pred_action(x)\n",
    "        pred_reward = self.pred_reward(x)\n",
    "        \n",
    "        return pred_state, pred_action, pred_reward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59314071",
   "metadata": {},
   "source": [
    "### **T - T**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bec8739d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class traj_transformer(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_layers = num_layers):\n",
    "        super(traj_transformer, self).__init__()\n",
    "        \n",
    "        # embedding\n",
    "        \n",
    "        self.embedding = input_embedding()\n",
    "        \n",
    "        # pos encoding\n",
    "        \n",
    "        self.pos_encoding = pos_encoding()\n",
    "        \n",
    "        # tranformer block\n",
    "        \n",
    "        self.layers = nn.ModuleList([\n",
    "            \n",
    "            transformer_block()\n",
    "            \n",
    "            for _ in range(num_layers)\n",
    "            \n",
    "        ])\n",
    "        \n",
    "        # policy head ?\n",
    "        \n",
    "        self.prediction = prediction_net()\n",
    "        \n",
    "        # normalization\n",
    "        \n",
    "        self.apply(self.init_weight)\n",
    "        \n",
    "    def forward(self, state, action, reward):\n",
    "        \n",
    "        # get embedding\n",
    "        \n",
    "        state_embed, action_embed, reward_embed = self.embedding.forward(state, action, reward)\n",
    "        \n",
    "        # positional encoding\n",
    "        \n",
    "        cat = torch.cat([state_embed, action_embed, reward_embed], dim = 1)\n",
    "        \n",
    "        x = self.pos_encoding.forward(cat)\n",
    "        \n",
    "        # transformer block\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            \n",
    "            x = layer(x)\n",
    "\n",
    "        pred_state, pred_action, pred_reward = self.prediction.forward(x)\n",
    "\n",
    "        return pred_state, pred_action, pred_reward\n",
    "        \n",
    "    def init_weight(self, m):\n",
    "        \n",
    "        if isinstance(m, nn.Linear):\n",
    "            \n",
    "            nn.init.orthogonal_(m.weight)\n",
    "            \n",
    "            if m.bias is not None:\n",
    "                \n",
    "                nn.init.zeros_(m.bias)\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc58bf4",
   "metadata": {},
   "source": [
    "### **SETUP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c4bb360e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traj_transformer(\n",
      "  (embedding): input_embedding(\n",
      "    (state_embed): Sequential(\n",
      "      (0): Linear(in_features=11, out_features=128, bias=True)\n",
      "      (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (2): SiLU()\n",
      "      (3): Linear(in_features=128, out_features=256, bias=True)\n",
      "      (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (5): SiLU()\n",
      "      (6): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (7): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (8): SiLU()\n",
      "      (9): Linear(in_features=256, out_features=128, bias=True)\n",
      "    )\n",
      "    (action_embed): Sequential(\n",
      "      (0): Linear(in_features=3, out_features=128, bias=True)\n",
      "      (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (2): SiLU()\n",
      "      (3): Linear(in_features=128, out_features=256, bias=True)\n",
      "      (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (5): SiLU()\n",
      "      (6): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (7): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (8): SiLU()\n",
      "      (9): Linear(in_features=256, out_features=128, bias=True)\n",
      "    )\n",
      "    (reward_embed): Sequential(\n",
      "      (0): Linear(in_features=1, out_features=128, bias=True)\n",
      "      (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (2): SiLU()\n",
      "      (3): Linear(in_features=128, out_features=256, bias=True)\n",
      "      (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (5): SiLU()\n",
      "      (6): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (7): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (8): SiLU()\n",
      "      (9): Linear(in_features=256, out_features=128, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (pos_encoding): pos_encoding()\n",
      "  (layers): ModuleList(\n",
      "    (0-7): 8 x transformer_block(\n",
      "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (mha): Multi_Head_Attention(\n",
      "        (Q): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (V): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (K): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "      )\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (ffn): feed_forward_network(\n",
      "        (pre_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (ffn): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "          (1): SiLU()\n",
      "          (2): Linear(in_features=256, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (drop2): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (prediction): prediction_net(\n",
      "    (pred_state): Sequential(\n",
      "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (1): SiLU()\n",
      "      (2): Linear(in_features=128, out_features=11, bias=True)\n",
      "    )\n",
      "    (pred_action): Sequential(\n",
      "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (1): SiLU()\n",
      "      (2): Linear(in_features=128, out_features=3, bias=True)\n",
      "    )\n",
      "    (pred_reward): Sequential(\n",
      "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (1): SiLU()\n",
      "      (2): Linear(in_features=128, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "TRAJECTORY_TRANSFORMER = traj_transformer().to(device)\n",
    "\n",
    "print(TRAJECTORY_TRANSFORMER)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3046d8c4",
   "metadata": {},
   "source": [
    "### **NO. OF LEARNABLE PARAMS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2d42b39c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of learnable params: 1516687\n"
     ]
    }
   ],
   "source": [
    "params = [p.numel() for p in TRAJECTORY_TRANSFORMER.parameters()]\n",
    "\n",
    "print(f'Number of learnable params: {np.sum(params)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8a92f0",
   "metadata": {},
   "source": [
    "### **OPTIMIZER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0dc759e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr\n",
    "\n",
    "prediction_lr = 3e-4\n",
    "embed_lr = 1e-4\n",
    "transformer_lr = 1e-5\n",
    "\n",
    "# iterations\n",
    "\n",
    "total_iters = 10_000\n",
    "T_max = 50_000\n",
    "epochs = 500\n",
    "\n",
    "# params\n",
    "\n",
    "prediction_params = TRAJECTORY_TRANSFORMER.prediction.parameters()\n",
    "embed_params = TRAJECTORY_TRANSFORMER.embedding.parameters()\n",
    "transformer_params = TRAJECTORY_TRANSFORMER.layers.parameters()\n",
    "\n",
    "# optimizer\n",
    "\n",
    "OPTIMIZER = optim.AdamW([\n",
    "    \n",
    "    {'params': prediction_params, 'lr': prediction_lr, 'weight_decay': 0},\n",
    "    {'params': embed_params, 'lr': embed_lr, 'weight_decay': 1e-6},\n",
    "    {'params': transformer_params, 'lr': transformer_lr, 'weight_decay': 1e-6}\n",
    "    \n",
    "])\n",
    "\n",
    "# Scheduler\n",
    "\n",
    "warmup = optim.lr_scheduler.LinearLR(OPTIMIZER, start_factor = 0.5, end_factor = 1, total_iters = total_iters)\n",
    "cosine = optim.lr_scheduler.CosineAnnealingLR(OPTIMIZER, T_max = T_max - total_iters, eta_min = 1e-5)\n",
    "\n",
    "SCHEDULER = optim.lr_scheduler.SequentialLR(OPTIMIZER, [warmup, cosine], milestones = [total_iters])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab661d7",
   "metadata": {},
   "source": [
    "### **TRAINING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8457defa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(loader, epochs = epochs):\n",
    "    \n",
    "    import os\n",
    "\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "    os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "        \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        \n",
    "        for states, actions, rewards, next_obs in loader:\n",
    "            \n",
    "            pred_state, pred_action, pred_reward = TRAJECTORY_TRANSFORMER.forward(states, actions, rewards)\n",
    "            \n",
    "            B, seq_length, act_dim = pred_action.size()\n",
    "\n",
    "            action_indices = torch.arange(1, seq_length, 3, device = pred_action.device)\n",
    "            state_indices = torch.arange(0, seq_length, 3, device = pred_state.device)\n",
    "            reward_indices = torch.arange(2, seq_length, 3, device = pred_reward.device)\n",
    "        \n",
    "            pred_states  = pred_state[:, state_indices, :]\n",
    "            pred_actions = pred_action[:, action_indices, :]\n",
    "            pred_rewards = pred_reward[:, reward_indices, :]\n",
    "\n",
    "            loss_1 = F.smooth_l1_loss(pred_states, states)\n",
    "            loss_2 = F.smooth_l1_loss(pred_rewards, rewards)\n",
    "            loss_3 = F.smooth_l1_loss(pred_actions, actions)\n",
    "            \n",
    "            batch_loss = loss_1 + loss_2 + loss_3\n",
    "            \n",
    "            OPTIMIZER.zero_grad()\n",
    "            batch_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(TRAJECTORY_TRANSFORMER.parameters(), max_norm = 0.5)\n",
    "            OPTIMIZER.step()\n",
    "            SCHEDULER.step()\n",
    "            \n",
    "            total_loss += batch_loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(loader)\n",
    "        \n",
    "        writer.add_scalar('Avg TT Loss', avg_loss, epoch)\n",
    "        writer.flush()\n",
    "        \n",
    "        print(f'Epoch: {epoch} | loss: {avg_loss:.3f}')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f5b9ea1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | loss: 8.631\n",
      "Epoch: 1 | loss: 5.081\n",
      "Epoch: 2 | loss: 3.621\n",
      "Epoch: 3 | loss: 2.675\n",
      "Epoch: 4 | loss: 2.138\n",
      "Epoch: 5 | loss: 1.798\n",
      "Epoch: 6 | loss: 1.583\n",
      "Epoch: 7 | loss: 1.445\n",
      "Epoch: 8 | loss: 1.342\n",
      "Epoch: 9 | loss: 1.279\n",
      "Epoch: 10 | loss: 1.215\n",
      "Epoch: 11 | loss: 1.139\n",
      "Epoch: 12 | loss: 1.098\n",
      "Epoch: 13 | loss: 1.053\n",
      "Epoch: 14 | loss: 1.007\n",
      "Epoch: 15 | loss: 0.962\n",
      "Epoch: 16 | loss: 0.917\n",
      "Epoch: 17 | loss: 0.861\n",
      "Epoch: 18 | loss: 0.821\n",
      "Epoch: 19 | loss: 0.793\n",
      "Epoch: 20 | loss: 0.761\n",
      "Epoch: 21 | loss: 0.736\n",
      "Epoch: 22 | loss: 0.714\n",
      "Epoch: 23 | loss: 0.682\n",
      "Epoch: 24 | loss: 0.665\n",
      "Epoch: 25 | loss: 0.656\n",
      "Epoch: 26 | loss: 0.652\n",
      "Epoch: 27 | loss: 0.646\n",
      "Epoch: 28 | loss: 0.640\n",
      "Epoch: 29 | loss: 0.628\n",
      "Epoch: 30 | loss: 0.621\n",
      "Epoch: 31 | loss: 0.617\n",
      "Epoch: 32 | loss: 0.610\n",
      "Epoch: 33 | loss: 0.603\n",
      "Epoch: 34 | loss: 0.594\n",
      "Epoch: 35 | loss: 0.593\n",
      "Epoch: 36 | loss: 0.582\n",
      "Epoch: 37 | loss: 0.573\n",
      "Epoch: 38 | loss: 0.579\n",
      "Epoch: 39 | loss: 0.575\n",
      "Epoch: 40 | loss: 0.566\n",
      "Epoch: 41 | loss: 0.560\n",
      "Epoch: 42 | loss: 0.554\n",
      "Epoch: 43 | loss: 0.549\n",
      "Epoch: 44 | loss: 0.545\n",
      "Epoch: 45 | loss: 0.541\n",
      "Epoch: 46 | loss: 0.540\n",
      "Epoch: 47 | loss: 0.530\n",
      "Epoch: 48 | loss: 0.527\n",
      "Epoch: 49 | loss: 0.523\n",
      "Epoch: 50 | loss: 0.516\n",
      "Epoch: 51 | loss: 0.509\n",
      "Epoch: 52 | loss: 0.502\n",
      "Epoch: 53 | loss: 0.503\n",
      "Epoch: 54 | loss: 0.498\n",
      "Epoch: 55 | loss: 0.495\n",
      "Epoch: 56 | loss: 0.487\n",
      "Epoch: 57 | loss: 0.483\n",
      "Epoch: 58 | loss: 0.483\n",
      "Epoch: 59 | loss: 0.476\n",
      "Epoch: 60 | loss: 0.477\n",
      "Epoch: 61 | loss: 0.469\n",
      "Epoch: 62 | loss: 0.465\n",
      "Epoch: 63 | loss: 0.461\n",
      "Epoch: 64 | loss: 0.457\n",
      "Epoch: 65 | loss: 0.456\n",
      "Epoch: 66 | loss: 0.450\n",
      "Epoch: 67 | loss: 0.448\n",
      "Epoch: 68 | loss: 0.440\n",
      "Epoch: 69 | loss: 0.438\n",
      "Epoch: 70 | loss: 0.435\n",
      "Epoch: 71 | loss: 0.432\n",
      "Epoch: 72 | loss: 0.422\n",
      "Epoch: 73 | loss: 0.417\n",
      "Epoch: 74 | loss: 0.414\n",
      "Epoch: 75 | loss: 0.406\n",
      "Epoch: 76 | loss: 0.402\n",
      "Epoch: 77 | loss: 0.393\n",
      "Epoch: 78 | loss: 0.389\n",
      "Epoch: 79 | loss: 0.390\n",
      "Epoch: 80 | loss: 0.379\n",
      "Epoch: 81 | loss: 0.381\n",
      "Epoch: 82 | loss: 0.371\n",
      "Epoch: 83 | loss: 0.371\n",
      "Epoch: 84 | loss: 0.365\n",
      "Epoch: 85 | loss: 0.369\n",
      "Epoch: 86 | loss: 0.363\n",
      "Epoch: 87 | loss: 0.358\n",
      "Epoch: 88 | loss: 0.353\n",
      "Epoch: 89 | loss: 0.347\n",
      "Epoch: 90 | loss: 0.346\n",
      "Epoch: 91 | loss: 0.342\n",
      "Epoch: 92 | loss: 0.342\n",
      "Epoch: 93 | loss: 0.336\n",
      "Epoch: 94 | loss: 0.331\n",
      "Epoch: 95 | loss: 0.332\n",
      "Epoch: 96 | loss: 0.329\n",
      "Epoch: 97 | loss: 0.324\n",
      "Epoch: 98 | loss: 0.322\n",
      "Epoch: 99 | loss: 0.318\n",
      "Epoch: 100 | loss: 0.316\n",
      "Epoch: 101 | loss: 0.311\n",
      "Epoch: 102 | loss: 0.312\n",
      "Epoch: 103 | loss: 0.309\n",
      "Epoch: 104 | loss: 0.308\n",
      "Epoch: 105 | loss: 0.304\n",
      "Epoch: 106 | loss: 0.304\n",
      "Epoch: 107 | loss: 0.301\n",
      "Epoch: 108 | loss: 0.298\n",
      "Epoch: 109 | loss: 0.296\n",
      "Epoch: 110 | loss: 0.294\n",
      "Epoch: 111 | loss: 0.292\n",
      "Epoch: 112 | loss: 0.288\n",
      "Epoch: 113 | loss: 0.287\n",
      "Epoch: 114 | loss: 0.287\n",
      "Epoch: 115 | loss: 0.289\n",
      "Epoch: 116 | loss: 0.282\n",
      "Epoch: 117 | loss: 0.282\n",
      "Epoch: 118 | loss: 0.281\n",
      "Epoch: 119 | loss: 0.278\n",
      "Epoch: 120 | loss: 0.278\n",
      "Epoch: 121 | loss: 0.273\n",
      "Epoch: 122 | loss: 0.272\n",
      "Epoch: 123 | loss: 0.271\n",
      "Epoch: 124 | loss: 0.268\n",
      "Epoch: 125 | loss: 0.267\n",
      "Epoch: 126 | loss: 0.266\n",
      "Epoch: 127 | loss: 0.267\n",
      "Epoch: 128 | loss: 0.264\n",
      "Epoch: 129 | loss: 0.263\n",
      "Epoch: 130 | loss: 0.261\n",
      "Epoch: 131 | loss: 0.258\n",
      "Epoch: 132 | loss: 0.258\n",
      "Epoch: 133 | loss: 0.256\n",
      "Epoch: 134 | loss: 0.253\n",
      "Epoch: 135 | loss: 0.254\n",
      "Epoch: 136 | loss: 0.252\n",
      "Epoch: 137 | loss: 0.248\n",
      "Epoch: 138 | loss: 0.248\n",
      "Epoch: 139 | loss: 0.249\n",
      "Epoch: 140 | loss: 0.246\n",
      "Epoch: 141 | loss: 0.247\n",
      "Epoch: 142 | loss: 0.244\n",
      "Epoch: 143 | loss: 0.244\n",
      "Epoch: 144 | loss: 0.246\n",
      "Epoch: 145 | loss: 0.241\n",
      "Epoch: 146 | loss: 0.241\n",
      "Epoch: 147 | loss: 0.240\n",
      "Epoch: 148 | loss: 0.237\n",
      "Epoch: 149 | loss: 0.239\n",
      "Epoch: 150 | loss: 0.238\n",
      "Epoch: 151 | loss: 0.239\n",
      "Epoch: 152 | loss: 0.237\n",
      "Epoch: 153 | loss: 0.235\n",
      "Epoch: 154 | loss: 0.233\n",
      "Epoch: 155 | loss: 0.233\n",
      "Epoch: 156 | loss: 0.237\n",
      "Epoch: 157 | loss: 0.230\n",
      "Epoch: 158 | loss: 0.230\n",
      "Epoch: 159 | loss: 0.229\n",
      "Epoch: 160 | loss: 0.229\n",
      "Epoch: 161 | loss: 0.226\n",
      "Epoch: 162 | loss: 0.226\n",
      "Epoch: 163 | loss: 0.225\n",
      "Epoch: 164 | loss: 0.225\n",
      "Epoch: 165 | loss: 0.225\n",
      "Epoch: 166 | loss: 0.222\n",
      "Epoch: 167 | loss: 0.224\n",
      "Epoch: 168 | loss: 0.220\n",
      "Epoch: 169 | loss: 0.220\n",
      "Epoch: 170 | loss: 0.219\n",
      "Epoch: 171 | loss: 0.219\n",
      "Epoch: 172 | loss: 0.216\n",
      "Epoch: 173 | loss: 0.218\n",
      "Epoch: 174 | loss: 0.218\n",
      "Epoch: 175 | loss: 0.216\n",
      "Epoch: 176 | loss: 0.214\n",
      "Epoch: 177 | loss: 0.213\n",
      "Epoch: 178 | loss: 0.214\n",
      "Epoch: 179 | loss: 0.210\n",
      "Epoch: 180 | loss: 0.210\n",
      "Epoch: 181 | loss: 0.208\n",
      "Epoch: 182 | loss: 0.210\n",
      "Epoch: 183 | loss: 0.207\n",
      "Epoch: 184 | loss: 0.207\n",
      "Epoch: 185 | loss: 0.206\n",
      "Epoch: 186 | loss: 0.209\n",
      "Epoch: 187 | loss: 0.205\n",
      "Epoch: 188 | loss: 0.205\n",
      "Epoch: 189 | loss: 0.205\n",
      "Epoch: 190 | loss: 0.209\n",
      "Epoch: 191 | loss: 0.205\n",
      "Epoch: 192 | loss: 0.203\n",
      "Epoch: 193 | loss: 0.202\n",
      "Epoch: 194 | loss: 0.201\n",
      "Epoch: 195 | loss: 0.200\n",
      "Epoch: 196 | loss: 0.199\n",
      "Epoch: 197 | loss: 0.199\n",
      "Epoch: 198 | loss: 0.198\n",
      "Epoch: 199 | loss: 0.199\n",
      "Epoch: 200 | loss: 0.197\n",
      "Epoch: 201 | loss: 0.196\n",
      "Epoch: 202 | loss: 0.196\n",
      "Epoch: 203 | loss: 0.195\n",
      "Epoch: 204 | loss: 0.195\n",
      "Epoch: 205 | loss: 0.195\n",
      "Epoch: 206 | loss: 0.192\n",
      "Epoch: 207 | loss: 0.195\n",
      "Epoch: 208 | loss: 0.193\n",
      "Epoch: 209 | loss: 0.192\n",
      "Epoch: 210 | loss: 0.189\n",
      "Epoch: 211 | loss: 0.190\n",
      "Epoch: 212 | loss: 0.192\n",
      "Epoch: 213 | loss: 0.191\n",
      "Epoch: 214 | loss: 0.189\n",
      "Epoch: 215 | loss: 0.188\n",
      "Epoch: 216 | loss: 0.188\n",
      "Epoch: 217 | loss: 0.186\n",
      "Epoch: 218 | loss: 0.187\n",
      "Epoch: 219 | loss: 0.184\n",
      "Epoch: 220 | loss: 0.184\n",
      "Epoch: 221 | loss: 0.185\n",
      "Epoch: 222 | loss: 0.185\n",
      "Epoch: 223 | loss: 0.184\n",
      "Epoch: 224 | loss: 0.183\n",
      "Epoch: 225 | loss: 0.181\n",
      "Epoch: 226 | loss: 0.182\n",
      "Epoch: 227 | loss: 0.181\n",
      "Epoch: 228 | loss: 0.181\n",
      "Epoch: 229 | loss: 0.181\n",
      "Epoch: 230 | loss: 0.182\n",
      "Epoch: 231 | loss: 0.179\n",
      "Epoch: 232 | loss: 0.181\n",
      "Epoch: 233 | loss: 0.181\n",
      "Epoch: 234 | loss: 0.178\n",
      "Epoch: 235 | loss: 0.177\n",
      "Epoch: 236 | loss: 0.176\n",
      "Epoch: 237 | loss: 0.178\n",
      "Epoch: 238 | loss: 0.177\n",
      "Epoch: 239 | loss: 0.177\n",
      "Epoch: 240 | loss: 0.174\n",
      "Epoch: 241 | loss: 0.174\n",
      "Epoch: 242 | loss: 0.174\n",
      "Epoch: 243 | loss: 0.175\n",
      "Epoch: 244 | loss: 0.173\n",
      "Epoch: 245 | loss: 0.173\n",
      "Epoch: 246 | loss: 0.173\n",
      "Epoch: 247 | loss: 0.172\n",
      "Epoch: 248 | loss: 0.170\n",
      "Epoch: 249 | loss: 0.172\n",
      "Epoch: 250 | loss: 0.170\n",
      "Epoch: 251 | loss: 0.169\n",
      "Epoch: 252 | loss: 0.170\n",
      "Epoch: 253 | loss: 0.167\n",
      "Epoch: 254 | loss: 0.169\n",
      "Epoch: 255 | loss: 0.169\n",
      "Epoch: 256 | loss: 0.168\n",
      "Epoch: 257 | loss: 0.167\n",
      "Epoch: 258 | loss: 0.168\n",
      "Epoch: 259 | loss: 0.166\n",
      "Epoch: 260 | loss: 0.166\n",
      "Epoch: 261 | loss: 0.165\n",
      "Epoch: 262 | loss: 0.166\n",
      "Epoch: 263 | loss: 0.165\n",
      "Epoch: 264 | loss: 0.163\n",
      "Epoch: 265 | loss: 0.163\n",
      "Epoch: 266 | loss: 0.163\n",
      "Epoch: 267 | loss: 0.164\n",
      "Epoch: 268 | loss: 0.163\n",
      "Epoch: 269 | loss: 0.162\n",
      "Epoch: 270 | loss: 0.162\n",
      "Epoch: 271 | loss: 0.162\n",
      "Epoch: 272 | loss: 0.162\n",
      "Epoch: 273 | loss: 0.163\n",
      "Epoch: 274 | loss: 0.161\n",
      "Epoch: 275 | loss: 0.160\n",
      "Epoch: 276 | loss: 0.161\n",
      "Epoch: 277 | loss: 0.160\n",
      "Epoch: 278 | loss: 0.159\n",
      "Epoch: 279 | loss: 0.159\n",
      "Epoch: 280 | loss: 0.157\n",
      "Epoch: 281 | loss: 0.158\n",
      "Epoch: 282 | loss: 0.157\n",
      "Epoch: 283 | loss: 0.158\n",
      "Epoch: 284 | loss: 0.158\n",
      "Epoch: 285 | loss: 0.156\n",
      "Epoch: 286 | loss: 0.155\n",
      "Epoch: 287 | loss: 0.156\n",
      "Epoch: 288 | loss: 0.156\n",
      "Epoch: 289 | loss: 0.157\n",
      "Epoch: 290 | loss: 0.154\n",
      "Epoch: 291 | loss: 0.154\n",
      "Epoch: 292 | loss: 0.154\n",
      "Epoch: 293 | loss: 0.154\n",
      "Epoch: 294 | loss: 0.153\n",
      "Epoch: 295 | loss: 0.154\n",
      "Epoch: 296 | loss: 0.154\n",
      "Epoch: 297 | loss: 0.153\n",
      "Epoch: 298 | loss: 0.152\n",
      "Epoch: 299 | loss: 0.152\n",
      "Epoch: 300 | loss: 0.151\n",
      "Epoch: 301 | loss: 0.152\n",
      "Epoch: 302 | loss: 0.150\n",
      "Epoch: 303 | loss: 0.150\n",
      "Epoch: 304 | loss: 0.150\n",
      "Epoch: 305 | loss: 0.150\n",
      "Epoch: 306 | loss: 0.149\n",
      "Epoch: 307 | loss: 0.149\n",
      "Epoch: 308 | loss: 0.149\n",
      "Epoch: 309 | loss: 0.148\n",
      "Epoch: 310 | loss: 0.148\n",
      "Epoch: 311 | loss: 0.147\n",
      "Epoch: 312 | loss: 0.147\n",
      "Epoch: 313 | loss: 0.147\n",
      "Epoch: 314 | loss: 0.147\n",
      "Epoch: 315 | loss: 0.145\n",
      "Epoch: 316 | loss: 0.147\n",
      "Epoch: 317 | loss: 0.145\n",
      "Epoch: 318 | loss: 0.146\n",
      "Epoch: 319 | loss: 0.146\n",
      "Epoch: 320 | loss: 0.146\n",
      "Epoch: 321 | loss: 0.145\n",
      "Epoch: 322 | loss: 0.145\n",
      "Epoch: 323 | loss: 0.146\n",
      "Epoch: 324 | loss: 0.145\n",
      "Epoch: 325 | loss: 0.144\n",
      "Epoch: 326 | loss: 0.144\n",
      "Epoch: 327 | loss: 0.145\n",
      "Epoch: 328 | loss: 0.144\n",
      "Epoch: 329 | loss: 0.144\n",
      "Epoch: 330 | loss: 0.144\n",
      "Epoch: 331 | loss: 0.141\n",
      "Epoch: 332 | loss: 0.142\n",
      "Epoch: 333 | loss: 0.141\n",
      "Epoch: 334 | loss: 0.141\n",
      "Epoch: 335 | loss: 0.140\n",
      "Epoch: 336 | loss: 0.141\n",
      "Epoch: 337 | loss: 0.141\n",
      "Epoch: 338 | loss: 0.140\n",
      "Epoch: 339 | loss: 0.140\n",
      "Epoch: 340 | loss: 0.140\n",
      "Epoch: 341 | loss: 0.141\n",
      "Epoch: 342 | loss: 0.139\n",
      "Epoch: 343 | loss: 0.139\n",
      "Epoch: 344 | loss: 0.139\n",
      "Epoch: 345 | loss: 0.137\n",
      "Epoch: 346 | loss: 0.138\n",
      "Epoch: 347 | loss: 0.138\n",
      "Epoch: 348 | loss: 0.139\n",
      "Epoch: 349 | loss: 0.137\n",
      "Epoch: 350 | loss: 0.137\n",
      "Epoch: 351 | loss: 0.137\n",
      "Epoch: 352 | loss: 0.138\n",
      "Epoch: 353 | loss: 0.138\n",
      "Epoch: 354 | loss: 0.136\n",
      "Epoch: 355 | loss: 0.137\n",
      "Epoch: 356 | loss: 0.136\n",
      "Epoch: 357 | loss: 0.136\n",
      "Epoch: 358 | loss: 0.137\n",
      "Epoch: 359 | loss: 0.135\n",
      "Epoch: 360 | loss: 0.134\n",
      "Epoch: 361 | loss: 0.135\n",
      "Epoch: 362 | loss: 0.134\n",
      "Epoch: 363 | loss: 0.135\n",
      "Epoch: 364 | loss: 0.135\n",
      "Epoch: 365 | loss: 0.134\n",
      "Epoch: 366 | loss: 0.133\n",
      "Epoch: 367 | loss: 0.133\n",
      "Epoch: 368 | loss: 0.134\n",
      "Epoch: 369 | loss: 0.133\n",
      "Epoch: 370 | loss: 0.133\n",
      "Epoch: 371 | loss: 0.133\n",
      "Epoch: 372 | loss: 0.132\n",
      "Epoch: 373 | loss: 0.131\n",
      "Epoch: 374 | loss: 0.132\n",
      "Epoch: 375 | loss: 0.132\n",
      "Epoch: 376 | loss: 0.132\n",
      "Epoch: 377 | loss: 0.132\n",
      "Epoch: 378 | loss: 0.131\n",
      "Epoch: 379 | loss: 0.132\n",
      "Epoch: 380 | loss: 0.133\n",
      "Epoch: 381 | loss: 0.133\n",
      "Epoch: 382 | loss: 0.132\n",
      "Epoch: 383 | loss: 0.131\n",
      "Epoch: 384 | loss: 0.130\n",
      "Epoch: 385 | loss: 0.132\n",
      "Epoch: 386 | loss: 0.135\n",
      "Epoch: 387 | loss: 0.131\n",
      "Epoch: 388 | loss: 0.130\n",
      "Epoch: 389 | loss: 0.128\n",
      "Epoch: 390 | loss: 0.127\n",
      "Epoch: 391 | loss: 0.127\n",
      "Epoch: 392 | loss: 0.128\n",
      "Epoch: 393 | loss: 0.128\n",
      "Epoch: 394 | loss: 0.127\n",
      "Epoch: 395 | loss: 0.127\n",
      "Epoch: 396 | loss: 0.126\n",
      "Epoch: 397 | loss: 0.126\n",
      "Epoch: 398 | loss: 0.126\n",
      "Epoch: 399 | loss: 0.126\n",
      "Epoch: 400 | loss: 0.126\n",
      "Epoch: 401 | loss: 0.126\n",
      "Epoch: 402 | loss: 0.126\n",
      "Epoch: 403 | loss: 0.127\n",
      "Epoch: 404 | loss: 0.126\n",
      "Epoch: 405 | loss: 0.126\n",
      "Epoch: 406 | loss: 0.126\n",
      "Epoch: 407 | loss: 0.125\n",
      "Epoch: 408 | loss: 0.125\n",
      "Epoch: 409 | loss: 0.124\n",
      "Epoch: 410 | loss: 0.125\n",
      "Epoch: 411 | loss: 0.126\n",
      "Epoch: 412 | loss: 0.123\n",
      "Epoch: 413 | loss: 0.124\n",
      "Epoch: 414 | loss: 0.123\n",
      "Epoch: 415 | loss: 0.122\n",
      "Epoch: 416 | loss: 0.124\n",
      "Epoch: 417 | loss: 0.124\n",
      "Epoch: 418 | loss: 0.124\n",
      "Epoch: 419 | loss: 0.122\n",
      "Epoch: 420 | loss: 0.123\n",
      "Epoch: 421 | loss: 0.122\n",
      "Epoch: 422 | loss: 0.123\n",
      "Epoch: 423 | loss: 0.122\n",
      "Epoch: 424 | loss: 0.122\n",
      "Epoch: 425 | loss: 0.122\n",
      "Epoch: 426 | loss: 0.122\n",
      "Epoch: 427 | loss: 0.122\n",
      "Epoch: 428 | loss: 0.122\n",
      "Epoch: 429 | loss: 0.122\n",
      "Epoch: 430 | loss: 0.121\n",
      "Epoch: 431 | loss: 0.121\n",
      "Epoch: 432 | loss: 0.121\n",
      "Epoch: 433 | loss: 0.120\n",
      "Epoch: 434 | loss: 0.121\n",
      "Epoch: 435 | loss: 0.120\n",
      "Epoch: 436 | loss: 0.119\n",
      "Epoch: 437 | loss: 0.120\n",
      "Epoch: 438 | loss: 0.119\n",
      "Epoch: 439 | loss: 0.119\n",
      "Epoch: 440 | loss: 0.120\n",
      "Epoch: 441 | loss: 0.119\n",
      "Epoch: 442 | loss: 0.119\n",
      "Epoch: 443 | loss: 0.119\n",
      "Epoch: 444 | loss: 0.119\n",
      "Epoch: 445 | loss: 0.118\n",
      "Epoch: 446 | loss: 0.118\n",
      "Epoch: 447 | loss: 0.118\n",
      "Epoch: 448 | loss: 0.118\n",
      "Epoch: 449 | loss: 0.118\n",
      "Epoch: 450 | loss: 0.118\n",
      "Epoch: 451 | loss: 0.117\n",
      "Epoch: 452 | loss: 0.118\n",
      "Epoch: 453 | loss: 0.117\n",
      "Epoch: 454 | loss: 0.117\n",
      "Epoch: 455 | loss: 0.117\n",
      "Epoch: 456 | loss: 0.116\n",
      "Epoch: 457 | loss: 0.117\n",
      "Epoch: 458 | loss: 0.115\n",
      "Epoch: 459 | loss: 0.117\n",
      "Epoch: 460 | loss: 0.116\n",
      "Epoch: 461 | loss: 0.117\n",
      "Epoch: 462 | loss: 0.116\n",
      "Epoch: 463 | loss: 0.115\n",
      "Epoch: 464 | loss: 0.116\n",
      "Epoch: 465 | loss: 0.116\n",
      "Epoch: 466 | loss: 0.116\n",
      "Epoch: 467 | loss: 0.115\n",
      "Epoch: 468 | loss: 0.115\n",
      "Epoch: 469 | loss: 0.116\n",
      "Epoch: 470 | loss: 0.114\n",
      "Epoch: 471 | loss: 0.114\n",
      "Epoch: 472 | loss: 0.114\n",
      "Epoch: 473 | loss: 0.115\n",
      "Epoch: 474 | loss: 0.115\n",
      "Epoch: 475 | loss: 0.114\n",
      "Epoch: 476 | loss: 0.114\n",
      "Epoch: 477 | loss: 0.114\n",
      "Epoch: 478 | loss: 0.114\n",
      "Epoch: 479 | loss: 0.113\n",
      "Epoch: 480 | loss: 0.113\n",
      "Epoch: 481 | loss: 0.113\n",
      "Epoch: 482 | loss: 0.111\n",
      "Epoch: 483 | loss: 0.113\n",
      "Epoch: 484 | loss: 0.113\n",
      "Epoch: 485 | loss: 0.112\n",
      "Epoch: 486 | loss: 0.113\n",
      "Epoch: 487 | loss: 0.112\n",
      "Epoch: 488 | loss: 0.111\n",
      "Epoch: 489 | loss: 0.112\n",
      "Epoch: 490 | loss: 0.113\n",
      "Epoch: 491 | loss: 0.112\n",
      "Epoch: 492 | loss: 0.113\n",
      "Epoch: 493 | loss: 0.112\n",
      "Epoch: 494 | loss: 0.111\n",
      "Epoch: 495 | loss: 0.112\n",
      "Epoch: 496 | loss: 0.111\n",
      "Epoch: 497 | loss: 0.111\n",
      "Epoch: 498 | loss: 0.111\n",
      "Epoch: 499 | loss: 0.110\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_loop(data_loader)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
