{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ef0dc05",
   "metadata": {},
   "source": [
    "# **D-T** *(Decision Transformer)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0f98d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ade76bf",
   "metadata": {},
   "source": [
    "### **DEVICE HANDLING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "935e1c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cbb7ac",
   "metadata": {},
   "source": [
    "### **LOGGING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6b129b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(log_dir = '.runs/DT')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bd522e",
   "metadata": {},
   "source": [
    "### **DATA HANDLING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d6b88ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state dim: 11 | action dim: 3 | dataset size: 1000000 | max actions: 0.9999945163726807\n"
     ]
    }
   ],
   "source": [
    "file_path = r\"C:\\OFFLINE RL\\hopper_medium-v2.hdf5\"\n",
    "\n",
    "with h5py.File(file_path, 'r') as f:\n",
    "    \n",
    "    obs = np.array(f['observations'])\n",
    "    act = np.array(f['actions'])\n",
    "    rew = np.array(f['rewards'])\n",
    "    terminals = np.array(f['terminals'])\n",
    "    timeouts = np.array(f['timeouts'])\n",
    "\n",
    "done = terminals | timeouts\n",
    "    \n",
    "state_dim = obs.shape[1]\n",
    "action_dim = act.shape[1]\n",
    "max_action = np.abs(act).max()\n",
    "dataset_size = obs.shape[0]\n",
    "\n",
    "print(f'state dim: {state_dim} | action dim: {action_dim} | dataset size: {dataset_size} | max actions: {max_action}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102b216a",
   "metadata": {},
   "source": [
    "### **HYPER PARAMS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9032b048",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "head_1 = 64\n",
    "head_2 = 128\n",
    "head_3 = 128\n",
    "head_4 = 64\n",
    "\n",
    "num_heads = 2\n",
    "d_model = 128\n",
    "max_len = 10_000\n",
    "\n",
    "K = 64\n",
    "\n",
    "rtg_dim = 1\n",
    "dropout = 0.1\n",
    "num_layers = 8\n",
    "discount = 1.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660b51fe",
   "metadata": {},
   "source": [
    "### **CREATE SEQUENCE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c2a5d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_episodes(obs, act, rew, done = done):\n",
    "    \n",
    "    episodes = []\n",
    "    start = 0\n",
    "    \n",
    "    for i in range(len(obs)):\n",
    "        \n",
    "        if done[i] == 1:\n",
    "            \n",
    "            episodes.append((obs[start:i+1], act[start:i+1], rew[start:i+1]))\n",
    "            start = i + 1\n",
    "            \n",
    "    if start < len(obs):  # in case last traj doesnâ€™t end with terminal\n",
    "        \n",
    "        episodes.append((obs[start:], act[start:], rew[start:]))\n",
    "        \n",
    "    return episodes\n",
    "\n",
    "\n",
    "episodes = split_into_episodes(obs, act, rew)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086c0b6e",
   "metadata": {},
   "source": [
    "### **HOPPER DATASET**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0225cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HopperDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, episodes, K=20):\n",
    "        \n",
    "        \"\"\"\n",
    "        episodes: list of (obs, act, rew) trajectories\n",
    "        K: context length (sequence length)\n",
    "        \"\"\"\n",
    "        \n",
    "        self.episodes = episodes\n",
    "        self.K = K\n",
    "\n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.episodes)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        obs, act, rew = self.episodes[idx]\n",
    "\n",
    "        # pad or truncate to fixed K length\n",
    "        tlen = len(obs)\n",
    "        \n",
    "        if tlen >= self.K:\n",
    "            \n",
    "            start = np.random.randint(0, tlen - self.K + 1)\n",
    "            obs = obs[start:start+self.K]\n",
    "            act = act[start:start+self.K]\n",
    "            rew = rew[start:start+self.K]\n",
    "            \n",
    "        else:  # pad\n",
    "            \n",
    "            pad_len = self.K - tlen\n",
    "            obs = np.concatenate([obs, np.zeros((pad_len, obs.shape[1]))], axis=0)\n",
    "            act = np.concatenate([act, np.zeros((pad_len, act.shape[1]))], axis=0)\n",
    "            rew = np.concatenate([rew, np.zeros(pad_len)], axis=0)\n",
    "\n",
    "        return (\n",
    "            torch.tensor(obs, dtype=torch.float32, device = device),\n",
    "            torch.tensor(act, dtype=torch.float32, device = device),\n",
    "            torch.tensor(rew, dtype=torch.float32, device = device).unsqueeze(1),\n",
    "        )\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "# convert numpy array to tensor\n",
    "\n",
    "Hopper_Data = HopperDataset(episodes, K = K)\n",
    "\n",
    "train_data, test_data = train_test_split(Hopper_Data, test_size = 0.2, random_state = 42)\n",
    "Train_Loader, Test_Loader = DataLoader(train_data, batch_size, shuffle = True, drop_last = True), DataLoader(test_data, batch_size, shuffle = True, drop_last = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5b3fdc",
   "metadata": {},
   "source": [
    "### **INPUT EMBEDDINGS**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d4c3953",
   "metadata": {},
   "outputs": [],
   "source": [
    "class prepare_embeds(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_dim = state_dim, action_dim = action_dim, rtg_dim = rtg_dim, head_1 = head_1, head_2 = head_2, head_3 = head_3, head_4 = head_4):\n",
    "        super(prepare_embeds, self).__init__()\n",
    "        \n",
    "        # embed\n",
    "        \n",
    "        def create_embed(input_dim):\n",
    "            \n",
    "            mlp = nn.Sequential(\n",
    "                \n",
    "                nn.Linear(input_dim, head_1),\n",
    "                nn.LayerNorm(head_1),\n",
    "                nn.SiLU(),\n",
    "                \n",
    "                nn.Linear(head_1, head_2),\n",
    "                nn.LayerNorm(head_2),\n",
    "                nn.SiLU(),\n",
    "    \n",
    "                nn.Linear(head_2, head_3),\n",
    "                nn.SiLU()\n",
    "                \n",
    "            )\n",
    "            \n",
    "            return mlp\n",
    "        \n",
    "        # obs embed\n",
    "        \n",
    "        self.obs_embed = create_embed(state_dim)\n",
    "        self.act_embed = create_embed(action_dim)\n",
    "        self.rtg_embed = create_embed(rtg_dim)\n",
    "        \n",
    "    def forward(self, state, action, rtg):\n",
    "        \n",
    "        s_embed = self.obs_embed(state)\n",
    "        a_embed = self.act_embed(action)\n",
    "        rtg_embed = self.rtg_embed(rtg)\n",
    "        \n",
    "        return s_embed, a_embed, rtg_embed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a9ca68",
   "metadata": {},
   "source": [
    "### **SETUP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2962333b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepare_embeds(\n",
      "  (obs_embed): Sequential(\n",
      "    (0): Linear(in_features=11, out_features=64, bias=True)\n",
      "    (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    (2): SiLU()\n",
      "    (3): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (4): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (5): SiLU()\n",
      "    (6): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (7): SiLU()\n",
      "  )\n",
      "  (act_embed): Sequential(\n",
      "    (0): Linear(in_features=3, out_features=64, bias=True)\n",
      "    (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    (2): SiLU()\n",
      "    (3): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (4): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (5): SiLU()\n",
      "    (6): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (7): SiLU()\n",
      "  )\n",
      "  (rtg_embed): Sequential(\n",
      "    (0): Linear(in_features=1, out_features=64, bias=True)\n",
      "    (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    (2): SiLU()\n",
      "    (3): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (4): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (5): SiLU()\n",
      "    (6): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (7): SiLU()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "INPUT_EMBEDDING = prepare_embeds().to(device)\n",
    "\n",
    "print(INPUT_EMBEDDING)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2990591",
   "metadata": {},
   "source": [
    "### **POSITIONAL ENCODER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "453992b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class positional_encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, max_len = max_len, d_model = d_model):\n",
    "        super(positional_encoder, self).__init__()\n",
    "        \n",
    "        self.pos = torch.arange(0, max_len, dtype = torch.float32).unsqueeze(1)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        \n",
    "        self.div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10_000) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(self.pos * self.div_term)\n",
    "        pe[:, 1::2] = torch.cos(self.pos * self.div_term)\n",
    "        \n",
    "        pe = pe.unsqueeze(0)\n",
    "        \n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        seq_length = x.size(1)\n",
    "        \n",
    "        x = x + self.pe[:, :seq_length]\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8143d4",
   "metadata": {},
   "source": [
    "### **SET UP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a71e551e",
   "metadata": {},
   "outputs": [],
   "source": [
    "POSITIONAL_ENCODER = positional_encoder().to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d3eece",
   "metadata": {},
   "source": [
    "### **ATTENTION MECHANISM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ecda7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class single_dot_attention(nn.Module):\n",
    "    \n",
    "    def __init__(self, head_1 = head_1, head_2 = head_2):\n",
    "        super(single_dot_attention, self).__init__()\n",
    "        \n",
    "        \n",
    "        self.Q = nn.Linear(head_1, head_2)\n",
    "        self.K = nn.Linear(head_1, head_2)\n",
    "        self.V = nn.Linear(head_1, head_2)\n",
    "        \n",
    "        self.norm = nn.LayerNorm(head_2)\n",
    "        self.proj = nn.Linear(head_2, head_2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        Q = self.Q(x)\n",
    "        K = self.K(x)\n",
    "        V = self.V(x)\n",
    "        \n",
    "        score = torch.matmul(Q , K.transpose(-2, -1)) / math.sqrt(Q.size(-1))\n",
    "        \n",
    "        weight = F.softmax(score, dim = -1)\n",
    "        \n",
    "        attn_out = torch.matmul(weight, V)\n",
    "        \n",
    "        proj_out = self.norm(self.proj(attn_out))\n",
    "        \n",
    "        return proj_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c52904",
   "metadata": {},
   "source": [
    "### **SETUP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25f0e18a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "single_dot_attention(\n",
      "  (Q): Linear(in_features=64, out_features=128, bias=True)\n",
      "  (K): Linear(in_features=64, out_features=128, bias=True)\n",
      "  (V): Linear(in_features=64, out_features=128, bias=True)\n",
      "  (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (proj): Linear(in_features=128, out_features=128, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "Single_Dot_Attention = single_dot_attention().to(device)\n",
    "\n",
    "print(Single_Dot_Attention)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a43dadc",
   "metadata": {},
   "source": [
    "### **CASUAL MASK**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed3a232b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def casual_mask(seq_length, device):\n",
    "    \n",
    "    mask = torch.tril(torch.ones((seq_length, seq_length), device = device))\n",
    "    \n",
    "    return mask.unsqueeze(0).unsqueeze(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85061e9d",
   "metadata": {},
   "source": [
    "### **MULTI HEAD ATTENTION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce9f7293",
   "metadata": {},
   "outputs": [],
   "source": [
    "class multi_head_attention(nn.Module):\n",
    "    \n",
    "    def __init__(self, head_2 = head_2, num_heads = num_heads):\n",
    "        super(multi_head_attention, self).__init__()\n",
    "        \n",
    "        # per head dim\n",
    "        \n",
    "        assert head_2 % num_heads == 0\n",
    "        \n",
    "        self.head_dim = head_2 // num_heads\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        # project Q, K, V\n",
    "        \n",
    "        self.Q = nn.Linear(head_2, head_2)\n",
    "        self.V = nn.Linear(head_2, head_2)\n",
    "        self.K = nn.Linear(head_2, head_2)\n",
    "        \n",
    "        # final projection\n",
    "        \n",
    "        self.norm = nn.LayerNorm(head_2)\n",
    "        self.proj = nn.Linear(head_2, head_2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # get dimension : [ batch, time , dim ]\n",
    "        \n",
    "        B, T, D = x.size()\n",
    "        \n",
    "        # project\n",
    "        \n",
    "        Q = self.Q(x)\n",
    "        V = self.V(x)\n",
    "        K = self.K(x)\n",
    "        \n",
    "        # nor transform: [ batch, time, num head, neuros per head]\n",
    "        \n",
    "        Q = Q.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = K.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = V.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # get scores\n",
    "        \n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / (math.sqrt(self.head_dim))\n",
    "        \n",
    "        mask = casual_mask(T, x.device)\n",
    "\n",
    "        scores = scores.masked_fill(mask == 0, -torch.inf)\n",
    "        \n",
    "        weights = F.softmax(scores, dim = -1)\n",
    "        \n",
    "        attn = torch.matmul(weights, V)\n",
    "        \n",
    "        # now transform again : [ batch, time, num heads * neurons per head ]\n",
    "        \n",
    "        attn = attn.transpose(1, 2).contiguous().view(B, T, D)\n",
    "        \n",
    "        # final pr0jection\n",
    "        \n",
    "        out = self.norm(x + self.proj(attn))\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93c9cb2",
   "metadata": {},
   "source": [
    "### **SETUP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5875e5a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multi_head_attention(\n",
      "  (Q): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (V): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (K): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (proj): Linear(in_features=128, out_features=128, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "Multi_Head_Attention = multi_head_attention().to(device)\n",
    "\n",
    "print(Multi_Head_Attention)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0270b2",
   "metadata": {},
   "source": [
    "### **FEED FORWARD NN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "201c5552",
   "metadata": {},
   "outputs": [],
   "source": [
    "class feed_forward_network(nn.Module):\n",
    "    \n",
    "    def __init__(self, head_2 = head_2, head_3 = head_3, head_4 = head_4):\n",
    "        super(feed_forward_network, self).__init__()\n",
    "        \n",
    "        # Pre norm layer\n",
    "        \n",
    "        self.pre_norm = nn.LayerNorm(head_2)\n",
    "        \n",
    "        # mlp \n",
    "        \n",
    "        self.mlp = nn.Sequential(\n",
    "        \n",
    "            nn.Linear(head_2, head_3),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.Linear(head_3, head_4),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.Linear(head_4, head_2)\n",
    "            \n",
    "        )\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # pre norm\n",
    "        \n",
    "        pre_norm = self.pre_norm(x)\n",
    "        \n",
    "        # mlp\n",
    "        \n",
    "        ffn = self.mlp(pre_norm)\n",
    "        \n",
    "        # residula network\n",
    "\n",
    "        out = x + ffn\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe7db89",
   "metadata": {},
   "source": [
    "### **SETUP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a9544cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feed_forward_network(\n",
      "  (pre_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (1): SiLU()\n",
      "    (2): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (3): SiLU()\n",
      "    (4): Linear(in_features=64, out_features=128, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "FFN = feed_forward_network().to(device)\n",
    "\n",
    "print(FFN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5081c148",
   "metadata": {},
   "source": [
    "### **TRANSFORMER BLOCK**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3d32440e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class transformer_block(nn.Module):\n",
    "    \n",
    "    def __init__(self, dropout = dropout, head_2 = head_2):\n",
    "        super(transformer_block, self).__init__()\n",
    "\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(head_2)\n",
    "        self.multi_head = multi_head_attention()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        \n",
    "        self.norm2 = nn.LayerNorm(head_2)\n",
    "        self.ffn = feed_forward_network()\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        norm1 = self.norm1(x)\n",
    "        attn = self.multi_head(norm1)\n",
    "        x = x + self.dropout1(attn)\n",
    "        \n",
    "        ffn = self.ffn(self.norm2(x))\n",
    "        x = x + self.dropout2(ffn)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067263a3",
   "metadata": {},
   "source": [
    "### **SET UP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "30462872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer_block(\n",
      "  (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (multi_head): multi_head_attention(\n",
      "    (Q): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (V): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (K): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "  )\n",
      "  (dropout1): Dropout(p=0.1, inplace=False)\n",
      "  (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (ffn): feed_forward_network(\n",
      "    (pre_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (mlp): Sequential(\n",
      "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (1): SiLU()\n",
      "      (2): Linear(in_features=128, out_features=64, bias=True)\n",
      "      (3): SiLU()\n",
      "      (4): Linear(in_features=64, out_features=128, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (dropout2): Dropout(p=0.1, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "Transformer_Block = transformer_block(dropout)\n",
    "\n",
    "print(Transformer_Block)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d4d4f8",
   "metadata": {},
   "source": [
    "### **ACTION HEAD**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c5da3cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class policy_net(nn.Module):\n",
    "    \n",
    "    def __init__(self, head_2 = head_2, action_dim = action_dim, max_action = max_action):\n",
    "        super(policy_net, self).__init__()\n",
    "        \n",
    "        self.mlp = nn.Sequential(\n",
    "            \n",
    "            nn.Linear(head_2, head_2),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(head_2, action_dim)\n",
    "            \n",
    "        )\n",
    "\n",
    "        # max action\n",
    "        \n",
    "        self.max_action = max_action\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        action = self.mlp(x)\n",
    "        action = action * self.max_action\n",
    "        \n",
    "        return action\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cdcc62",
   "metadata": {},
   "source": [
    "### **COMPLETE DT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "45d5a703",
   "metadata": {},
   "outputs": [],
   "source": [
    "class decision_transformer(nn.ModuleList):\n",
    "    \n",
    "    def __init__(self, num_layers = num_layers):\n",
    "        super(decision_transformer, self).__init__()\n",
    "        \n",
    "        # embeddings\n",
    "        \n",
    "        self.embedding = prepare_embeds()\n",
    "        \n",
    "        # positional encodings\n",
    "        \n",
    "        self.pos_encodings = positional_encoder()\n",
    "        \n",
    "        # Encoder block\n",
    "        \n",
    "        self.layers = nn.ModuleList([\n",
    "            \n",
    "            transformer_block().to(device)\n",
    "            for _ in range(num_layers)\n",
    "            \n",
    "        ])\n",
    "        \n",
    "        # policy\n",
    "        \n",
    "        self.policy = policy_net()\n",
    "        \n",
    "        # normalization\n",
    "        \n",
    "        self.apply(self.init_weight)\n",
    "        \n",
    "    def forward(self, rtg, state, action):\n",
    "        \n",
    "        # Get embeddings\n",
    "        \n",
    "        rtg_embed, state_embed, action_embed = self.embedding.forward(state = state, action = action, rtg = rtg)\n",
    "\n",
    "        # postions\n",
    "        \n",
    "        cat = torch.cat([ rtg_embed, state_embed, action_embed ], dim = 1)\n",
    "                \n",
    "        x = self.pos_encodings.forward(cat)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            \n",
    "            x = layer(x)\n",
    "            \n",
    "        action = self.policy(x)\n",
    "            \n",
    "        return action\n",
    "    \n",
    "    def init_weight(self, m):\n",
    "        \n",
    "        if isinstance(m, nn.Linear):\n",
    "            \n",
    "            nn.init.orthogonal_(m.weight)\n",
    "            \n",
    "            if m.bias is not None:\n",
    "                \n",
    "                nn.init.zeros_(m.bias)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c17ed6",
   "metadata": {},
   "source": [
    "### **SET UP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "411c9e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decision_transformer(\n",
      "  (0): prepare_embeds(\n",
      "    (obs_embed): Sequential(\n",
      "      (0): Linear(in_features=11, out_features=64, bias=True)\n",
      "      (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (2): SiLU()\n",
      "      (3): Linear(in_features=64, out_features=128, bias=True)\n",
      "      (4): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (5): SiLU()\n",
      "      (6): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (7): SiLU()\n",
      "    )\n",
      "    (act_embed): Sequential(\n",
      "      (0): Linear(in_features=3, out_features=64, bias=True)\n",
      "      (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (2): SiLU()\n",
      "      (3): Linear(in_features=64, out_features=128, bias=True)\n",
      "      (4): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (5): SiLU()\n",
      "      (6): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (7): SiLU()\n",
      "    )\n",
      "    (rtg_embed): Sequential(\n",
      "      (0): Linear(in_features=1, out_features=64, bias=True)\n",
      "      (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (2): SiLU()\n",
      "      (3): Linear(in_features=64, out_features=128, bias=True)\n",
      "      (4): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (5): SiLU()\n",
      "      (6): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (7): SiLU()\n",
      "    )\n",
      "  )\n",
      "  (1): positional_encoder()\n",
      "  (2): ModuleList(\n",
      "    (0-7): 8 x transformer_block(\n",
      "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (multi_head): multi_head_attention(\n",
      "        (Q): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (V): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (K): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "      )\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (ffn): feed_forward_network(\n",
      "        (pre_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): SiLU()\n",
      "          (2): Linear(in_features=128, out_features=64, bias=True)\n",
      "          (3): SiLU()\n",
      "          (4): Linear(in_features=64, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (3): policy_net(\n",
      "    (mlp): Sequential(\n",
      "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (1): SiLU()\n",
      "      (2): Linear(in_features=128, out_features=3, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "Decision_Transformer = decision_transformer().to(device)\n",
    "\n",
    "print(Decision_Transformer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f2a622",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = [p.numel() for p in Decision_Transformer.parameters()]\n",
    "\n",
    "print(f'Number of learnable params: {count}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf64c1d",
   "metadata": {},
   "source": [
    "### **OPTIMIZER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ce3d79da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr\n",
    "\n",
    "policy_lr = 3e-4\n",
    "embed_lr = 1e-4\n",
    "transformer_lr = 1e-5\n",
    "\n",
    "# iteration\n",
    "\n",
    "total_iter = 50\n",
    "T_max = 500\n",
    "\n",
    "# params\n",
    "\n",
    "policy_params = Decision_Transformer.policy.parameters()\n",
    "embed_params = Decision_Transformer.embedding.parameters()\n",
    "tranformer_params = Decision_Transformer.layers.parameters()\n",
    "\n",
    "# optimizer\n",
    "\n",
    "OPTIMIZER = optim.AdamW([\n",
    "    \n",
    "    {'params': policy_params, 'lr': policy_lr, 'weight_decay': 0},\n",
    "    {'params': embed_params, 'lr': embed_lr, 'weight_decay': 1e-6},\n",
    "    {'params': tranformer_params, 'lr': transformer_lr, 'weight_decay': 1e-6}\n",
    "    \n",
    "])\n",
    "\n",
    "# Scheduler\n",
    "\n",
    "warmup = optim.lr_scheduler.ConstantLR(OPTIMIZER, factor = 0.5, total_iters = total_iter)\n",
    "cosine = optim.lr_scheduler.CosineAnnealingLR(OPTIMIZER, T_max = T_max - total_iter, eta_min = 1e-5)\n",
    "\n",
    "SCHEDULER = optim.lr_scheduler.SequentialLR(OPTIMIZER, [warmup, cosine], [total_iter])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d24d2cc",
   "metadata": {},
   "source": [
    "### **COMPUTE RTG**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "76123a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rtg(rewards, discount = discount):\n",
    "    \n",
    "    rtg = []\n",
    "    running = 0\n",
    "    \n",
    "    B, T, _ = rewards.size()\n",
    "    \n",
    "    for r in reversed(range(T)):\n",
    "        \n",
    "        running = rewards[:, r] + discount * running\n",
    "        \n",
    "        rtg.insert(0, running)\n",
    "        \n",
    "    rtg = torch.stack(rtg).to(device)\n",
    "    \n",
    "    rtg = rtg.transpose(1, 0)\n",
    "    \n",
    "    return rtg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c26d4f3",
   "metadata": {},
   "source": [
    "### **TRAINING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "be506822",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(epochs, loader, Decision_Transformer = Decision_Transformer, OPTIMIZER = OPTIMIZER, SCHEDULER = SCHEDULER): \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        \n",
    "        for states, true_action, rewards in loader:\n",
    "            \n",
    "            rtg = compute_rtg(rewards)\n",
    "            \n",
    "            pred_action = Decision_Transformer.forward(rtg, states, true_action[:, :-1])\n",
    "            \n",
    "            B, seq_length, act_dim = pred_action.size()\n",
    "\n",
    "            action_indices = torch.arange(2, seq_length, 3, device = pred_action.device)\n",
    "            \n",
    "            pred_action = pred_action[:, action_indices, :]\n",
    "            \n",
    "            loss = F.mse_loss(pred_action, true_action[:, 1:], reduction = 'mean')\n",
    "            \n",
    "            OPTIMIZER.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(Decision_Transformer.parameters(), max_norm = 0.5)\n",
    "            OPTIMIZER.step()\n",
    "            SCHEDULER.step()\n",
    "            \n",
    "            total_loss += loss\n",
    "            \n",
    "        avg_loss = total_loss / len(loader)\n",
    "        \n",
    "        writer.add_scalar('DT_LOSS', avg_loss, epoch)\n",
    "        \n",
    "        print(f'Epoch: {epoch} | avg loss: {avg_loss:.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d980aea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | avg loss: 87.017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kiosh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | avg loss: 21.315\n",
      "Epoch: 2 | avg loss: 6.830\n",
      "Epoch: 3 | avg loss: 3.228\n",
      "Epoch: 4 | avg loss: 1.890\n",
      "Epoch: 5 | avg loss: 1.257\n",
      "Epoch: 6 | avg loss: 0.964\n",
      "Epoch: 7 | avg loss: 0.777\n",
      "Epoch: 8 | avg loss: 0.667\n",
      "Epoch: 9 | avg loss: 0.605\n",
      "Epoch: 10 | avg loss: 0.560\n",
      "Epoch: 11 | avg loss: 0.531\n",
      "Epoch: 12 | avg loss: 0.503\n",
      "Epoch: 13 | avg loss: 0.483\n",
      "Epoch: 14 | avg loss: 0.469\n",
      "Epoch: 15 | avg loss: 0.456\n",
      "Epoch: 16 | avg loss: 0.446\n",
      "Epoch: 17 | avg loss: 0.441\n",
      "Epoch: 18 | avg loss: 0.434\n",
      "Epoch: 19 | avg loss: 0.428\n",
      "Epoch: 20 | avg loss: 0.422\n",
      "Epoch: 21 | avg loss: 0.416\n",
      "Epoch: 22 | avg loss: 0.408\n",
      "Epoch: 23 | avg loss: 0.401\n",
      "Epoch: 24 | avg loss: 0.390\n",
      "Epoch: 25 | avg loss: 0.380\n",
      "Epoch: 26 | avg loss: 0.372\n",
      "Epoch: 27 | avg loss: 0.363\n",
      "Epoch: 28 | avg loss: 0.355\n",
      "Epoch: 29 | avg loss: 0.347\n",
      "Epoch: 30 | avg loss: 0.342\n",
      "Epoch: 31 | avg loss: 0.337\n",
      "Epoch: 32 | avg loss: 0.332\n",
      "Epoch: 33 | avg loss: 0.327\n",
      "Epoch: 34 | avg loss: 0.323\n",
      "Epoch: 35 | avg loss: 0.318\n",
      "Epoch: 36 | avg loss: 0.314\n",
      "Epoch: 37 | avg loss: 0.310\n",
      "Epoch: 38 | avg loss: 0.307\n",
      "Epoch: 39 | avg loss: 0.303\n",
      "Epoch: 40 | avg loss: 0.300\n",
      "Epoch: 41 | avg loss: 0.297\n",
      "Epoch: 42 | avg loss: 0.294\n",
      "Epoch: 43 | avg loss: 0.292\n",
      "Epoch: 44 | avg loss: 0.290\n",
      "Epoch: 45 | avg loss: 0.288\n",
      "Epoch: 46 | avg loss: 0.287\n",
      "Epoch: 47 | avg loss: 0.285\n",
      "Epoch: 48 | avg loss: 0.284\n",
      "Epoch: 49 | avg loss: 0.283\n",
      "Epoch: 50 | avg loss: 0.281\n",
      "Epoch: 51 | avg loss: 0.281\n",
      "Epoch: 52 | avg loss: 0.279\n",
      "Epoch: 53 | avg loss: 0.279\n",
      "Epoch: 54 | avg loss: 0.277\n",
      "Epoch: 55 | avg loss: 0.277\n",
      "Epoch: 56 | avg loss: 0.275\n",
      "Epoch: 57 | avg loss: 0.274\n",
      "Epoch: 58 | avg loss: 0.273\n",
      "Epoch: 59 | avg loss: 0.272\n",
      "Epoch: 60 | avg loss: 0.271\n",
      "Epoch: 61 | avg loss: 0.270\n",
      "Epoch: 62 | avg loss: 0.268\n",
      "Epoch: 63 | avg loss: 0.268\n",
      "Epoch: 64 | avg loss: 0.268\n",
      "Epoch: 65 | avg loss: 0.265\n",
      "Epoch: 66 | avg loss: 0.265\n",
      "Epoch: 67 | avg loss: 0.264\n",
      "Epoch: 68 | avg loss: 0.263\n",
      "Epoch: 69 | avg loss: 0.262\n",
      "Epoch: 70 | avg loss: 0.262\n",
      "Epoch: 71 | avg loss: 0.262\n",
      "Epoch: 72 | avg loss: 0.260\n",
      "Epoch: 73 | avg loss: 0.258\n",
      "Epoch: 74 | avg loss: 0.258\n",
      "Epoch: 75 | avg loss: 0.257\n",
      "Epoch: 76 | avg loss: 0.256\n",
      "Epoch: 77 | avg loss: 0.255\n",
      "Epoch: 78 | avg loss: 0.254\n",
      "Epoch: 79 | avg loss: 0.253\n",
      "Epoch: 80 | avg loss: 0.252\n",
      "Epoch: 81 | avg loss: 0.252\n",
      "Epoch: 82 | avg loss: 0.250\n",
      "Epoch: 83 | avg loss: 0.249\n",
      "Epoch: 84 | avg loss: 0.249\n",
      "Epoch: 85 | avg loss: 0.248\n",
      "Epoch: 86 | avg loss: 0.247\n",
      "Epoch: 87 | avg loss: 0.246\n",
      "Epoch: 88 | avg loss: 0.246\n",
      "Epoch: 89 | avg loss: 0.246\n",
      "Epoch: 90 | avg loss: 0.245\n",
      "Epoch: 91 | avg loss: 0.244\n",
      "Epoch: 92 | avg loss: 0.243\n",
      "Epoch: 93 | avg loss: 0.243\n",
      "Epoch: 94 | avg loss: 0.242\n",
      "Epoch: 95 | avg loss: 0.242\n",
      "Epoch: 96 | avg loss: 0.241\n",
      "Epoch: 97 | avg loss: 0.241\n",
      "Epoch: 98 | avg loss: 0.240\n",
      "Epoch: 99 | avg loss: 0.240\n",
      "Epoch: 100 | avg loss: 0.239\n",
      "Epoch: 101 | avg loss: 0.238\n",
      "Epoch: 102 | avg loss: 0.238\n",
      "Epoch: 103 | avg loss: 0.237\n",
      "Epoch: 104 | avg loss: 0.236\n",
      "Epoch: 105 | avg loss: 0.235\n",
      "Epoch: 106 | avg loss: 0.235\n",
      "Epoch: 107 | avg loss: 0.233\n",
      "Epoch: 108 | avg loss: 0.232\n",
      "Epoch: 109 | avg loss: 0.232\n",
      "Epoch: 110 | avg loss: 0.232\n",
      "Epoch: 111 | avg loss: 0.230\n",
      "Epoch: 112 | avg loss: 0.230\n",
      "Epoch: 113 | avg loss: 0.229\n",
      "Epoch: 114 | avg loss: 0.228\n",
      "Epoch: 115 | avg loss: 0.227\n",
      "Epoch: 116 | avg loss: 0.226\n",
      "Epoch: 117 | avg loss: 0.225\n",
      "Epoch: 118 | avg loss: 0.224\n",
      "Epoch: 119 | avg loss: 0.224\n",
      "Epoch: 120 | avg loss: 0.223\n",
      "Epoch: 121 | avg loss: 0.222\n",
      "Epoch: 122 | avg loss: 0.222\n",
      "Epoch: 123 | avg loss: 0.221\n",
      "Epoch: 124 | avg loss: 0.220\n",
      "Epoch: 125 | avg loss: 0.220\n",
      "Epoch: 126 | avg loss: 0.220\n",
      "Epoch: 127 | avg loss: 0.219\n",
      "Epoch: 128 | avg loss: 0.218\n",
      "Epoch: 129 | avg loss: 0.218\n",
      "Epoch: 130 | avg loss: 0.218\n",
      "Epoch: 131 | avg loss: 0.217\n",
      "Epoch: 132 | avg loss: 0.216\n",
      "Epoch: 133 | avg loss: 0.216\n",
      "Epoch: 134 | avg loss: 0.215\n",
      "Epoch: 135 | avg loss: 0.217\n",
      "Epoch: 136 | avg loss: 0.215\n",
      "Epoch: 137 | avg loss: 0.214\n",
      "Epoch: 138 | avg loss: 0.213\n",
      "Epoch: 139 | avg loss: 0.213\n",
      "Epoch: 140 | avg loss: 0.212\n",
      "Epoch: 141 | avg loss: 0.211\n",
      "Epoch: 142 | avg loss: 0.210\n",
      "Epoch: 143 | avg loss: 0.209\n",
      "Epoch: 144 | avg loss: 0.208\n",
      "Epoch: 145 | avg loss: 0.208\n",
      "Epoch: 146 | avg loss: 0.207\n",
      "Epoch: 147 | avg loss: 0.206\n",
      "Epoch: 148 | avg loss: 0.205\n",
      "Epoch: 149 | avg loss: 0.205\n",
      "Epoch: 150 | avg loss: 0.204\n",
      "Epoch: 151 | avg loss: 0.203\n",
      "Epoch: 152 | avg loss: 0.202\n",
      "Epoch: 153 | avg loss: 0.201\n",
      "Epoch: 154 | avg loss: 0.201\n",
      "Epoch: 155 | avg loss: 0.200\n",
      "Epoch: 156 | avg loss: 0.199\n",
      "Epoch: 157 | avg loss: 0.199\n",
      "Epoch: 158 | avg loss: 0.198\n",
      "Epoch: 159 | avg loss: 0.198\n",
      "Epoch: 160 | avg loss: 0.197\n",
      "Epoch: 161 | avg loss: 0.197\n",
      "Epoch: 162 | avg loss: 0.197\n",
      "Epoch: 163 | avg loss: 0.196\n",
      "Epoch: 164 | avg loss: 0.196\n",
      "Epoch: 165 | avg loss: 0.195\n",
      "Epoch: 166 | avg loss: 0.194\n",
      "Epoch: 167 | avg loss: 0.196\n",
      "Epoch: 168 | avg loss: 0.194\n",
      "Epoch: 169 | avg loss: 0.193\n",
      "Epoch: 170 | avg loss: 0.192\n",
      "Epoch: 171 | avg loss: 0.191\n",
      "Epoch: 172 | avg loss: 0.191\n",
      "Epoch: 173 | avg loss: 0.190\n",
      "Epoch: 174 | avg loss: 0.189\n",
      "Epoch: 175 | avg loss: 0.189\n",
      "Epoch: 176 | avg loss: 0.188\n",
      "Epoch: 177 | avg loss: 0.187\n",
      "Epoch: 178 | avg loss: 0.186\n",
      "Epoch: 179 | avg loss: 0.185\n",
      "Epoch: 180 | avg loss: 0.185\n",
      "Epoch: 181 | avg loss: 0.184\n",
      "Epoch: 182 | avg loss: 0.183\n",
      "Epoch: 183 | avg loss: 0.182\n",
      "Epoch: 184 | avg loss: 0.181\n",
      "Epoch: 185 | avg loss: 0.180\n",
      "Epoch: 186 | avg loss: 0.180\n",
      "Epoch: 187 | avg loss: 0.179\n",
      "Epoch: 188 | avg loss: 0.178\n",
      "Epoch: 189 | avg loss: 0.177\n",
      "Epoch: 190 | avg loss: 0.177\n",
      "Epoch: 191 | avg loss: 0.176\n",
      "Epoch: 192 | avg loss: 0.176\n",
      "Epoch: 193 | avg loss: 0.175\n",
      "Epoch: 194 | avg loss: 0.175\n",
      "Epoch: 195 | avg loss: 0.174\n",
      "Epoch: 196 | avg loss: 0.174\n",
      "Epoch: 197 | avg loss: 0.173\n",
      "Epoch: 198 | avg loss: 0.173\n",
      "Epoch: 199 | avg loss: 0.173\n",
      "Epoch: 200 | avg loss: 0.172\n",
      "Epoch: 201 | avg loss: 0.172\n",
      "Epoch: 202 | avg loss: 0.171\n",
      "Epoch: 203 | avg loss: 0.171\n",
      "Epoch: 204 | avg loss: 0.171\n",
      "Epoch: 205 | avg loss: 0.170\n",
      "Epoch: 206 | avg loss: 0.169\n",
      "Epoch: 207 | avg loss: 0.169\n",
      "Epoch: 208 | avg loss: 0.167\n",
      "Epoch: 209 | avg loss: 0.167\n",
      "Epoch: 210 | avg loss: 0.167\n",
      "Epoch: 211 | avg loss: 0.165\n",
      "Epoch: 212 | avg loss: 0.165\n",
      "Epoch: 213 | avg loss: 0.164\n",
      "Epoch: 214 | avg loss: 0.163\n",
      "Epoch: 215 | avg loss: 0.163\n",
      "Epoch: 216 | avg loss: 0.162\n",
      "Epoch: 217 | avg loss: 0.161\n",
      "Epoch: 218 | avg loss: 0.161\n",
      "Epoch: 219 | avg loss: 0.160\n",
      "Epoch: 220 | avg loss: 0.159\n",
      "Epoch: 221 | avg loss: 0.159\n",
      "Epoch: 222 | avg loss: 0.158\n",
      "Epoch: 223 | avg loss: 0.158\n",
      "Epoch: 224 | avg loss: 0.158\n",
      "Epoch: 225 | avg loss: 0.157\n",
      "Epoch: 226 | avg loss: 0.157\n",
      "Epoch: 227 | avg loss: 0.156\n",
      "Epoch: 228 | avg loss: 0.156\n",
      "Epoch: 229 | avg loss: 0.156\n",
      "Epoch: 230 | avg loss: 0.156\n",
      "Epoch: 231 | avg loss: 0.155\n",
      "Epoch: 232 | avg loss: 0.155\n",
      "Epoch: 233 | avg loss: 0.155\n",
      "Epoch: 234 | avg loss: 0.154\n",
      "Epoch: 235 | avg loss: 0.154\n",
      "Epoch: 236 | avg loss: 0.154\n",
      "Epoch: 237 | avg loss: 0.153\n",
      "Epoch: 238 | avg loss: 0.152\n",
      "Epoch: 239 | avg loss: 0.152\n",
      "Epoch: 240 | avg loss: 0.151\n",
      "Epoch: 241 | avg loss: 0.152\n",
      "Epoch: 242 | avg loss: 0.150\n",
      "Epoch: 243 | avg loss: 0.150\n",
      "Epoch: 244 | avg loss: 0.149\n",
      "Epoch: 245 | avg loss: 0.149\n",
      "Epoch: 246 | avg loss: 0.148\n",
      "Epoch: 247 | avg loss: 0.148\n",
      "Epoch: 248 | avg loss: 0.147\n",
      "Epoch: 249 | avg loss: 0.146\n",
      "Epoch: 250 | avg loss: 0.145\n",
      "Epoch: 251 | avg loss: 0.145\n",
      "Epoch: 252 | avg loss: 0.144\n",
      "Epoch: 253 | avg loss: 0.144\n",
      "Epoch: 254 | avg loss: 0.143\n",
      "Epoch: 255 | avg loss: 0.143\n",
      "Epoch: 256 | avg loss: 0.142\n",
      "Epoch: 257 | avg loss: 0.142\n",
      "Epoch: 258 | avg loss: 0.142\n",
      "Epoch: 259 | avg loss: 0.141\n",
      "Epoch: 260 | avg loss: 0.141\n",
      "Epoch: 261 | avg loss: 0.141\n",
      "Epoch: 262 | avg loss: 0.141\n",
      "Epoch: 263 | avg loss: 0.140\n",
      "Epoch: 264 | avg loss: 0.140\n",
      "Epoch: 265 | avg loss: 0.141\n",
      "Epoch: 266 | avg loss: 0.139\n",
      "Epoch: 267 | avg loss: 0.139\n",
      "Epoch: 268 | avg loss: 0.140\n",
      "Epoch: 269 | avg loss: 0.140\n",
      "Epoch: 270 | avg loss: 0.138\n",
      "Epoch: 271 | avg loss: 0.138\n",
      "Epoch: 272 | avg loss: 0.137\n",
      "Epoch: 273 | avg loss: 0.137\n",
      "Epoch: 274 | avg loss: 0.137\n",
      "Epoch: 275 | avg loss: 0.136\n",
      "Epoch: 276 | avg loss: 0.136\n",
      "Epoch: 277 | avg loss: 0.136\n",
      "Epoch: 278 | avg loss: 0.135\n",
      "Epoch: 279 | avg loss: 0.134\n",
      "Epoch: 280 | avg loss: 0.134\n",
      "Epoch: 281 | avg loss: 0.133\n",
      "Epoch: 282 | avg loss: 0.133\n",
      "Epoch: 283 | avg loss: 0.132\n",
      "Epoch: 284 | avg loss: 0.131\n",
      "Epoch: 285 | avg loss: 0.131\n",
      "Epoch: 286 | avg loss: 0.130\n",
      "Epoch: 287 | avg loss: 0.130\n",
      "Epoch: 288 | avg loss: 0.129\n",
      "Epoch: 289 | avg loss: 0.129\n",
      "Epoch: 290 | avg loss: 0.128\n",
      "Epoch: 291 | avg loss: 0.128\n",
      "Epoch: 292 | avg loss: 0.128\n",
      "Epoch: 293 | avg loss: 0.127\n",
      "Epoch: 294 | avg loss: 0.128\n",
      "Epoch: 295 | avg loss: 0.127\n",
      "Epoch: 296 | avg loss: 0.127\n",
      "Epoch: 297 | avg loss: 0.128\n",
      "Epoch: 298 | avg loss: 0.127\n",
      "Epoch: 299 | avg loss: 0.127\n",
      "Epoch: 300 | avg loss: 0.126\n",
      "Epoch: 301 | avg loss: 0.126\n",
      "Epoch: 302 | avg loss: 0.126\n",
      "Epoch: 303 | avg loss: 0.125\n",
      "Epoch: 304 | avg loss: 0.125\n",
      "Epoch: 305 | avg loss: 0.124\n",
      "Epoch: 306 | avg loss: 0.124\n",
      "Epoch: 307 | avg loss: 0.124\n",
      "Epoch: 308 | avg loss: 0.123\n",
      "Epoch: 309 | avg loss: 0.123\n",
      "Epoch: 310 | avg loss: 0.122\n",
      "Epoch: 311 | avg loss: 0.122\n",
      "Epoch: 312 | avg loss: 0.122\n",
      "Epoch: 313 | avg loss: 0.121\n",
      "Epoch: 314 | avg loss: 0.120\n",
      "Epoch: 315 | avg loss: 0.119\n",
      "Epoch: 316 | avg loss: 0.119\n",
      "Epoch: 317 | avg loss: 0.118\n",
      "Epoch: 318 | avg loss: 0.118\n",
      "Epoch: 319 | avg loss: 0.117\n",
      "Epoch: 320 | avg loss: 0.117\n",
      "Epoch: 321 | avg loss: 0.116\n",
      "Epoch: 322 | avg loss: 0.116\n",
      "Epoch: 323 | avg loss: 0.115\n",
      "Epoch: 324 | avg loss: 0.115\n",
      "Epoch: 325 | avg loss: 0.115\n",
      "Epoch: 326 | avg loss: 0.114\n",
      "Epoch: 327 | avg loss: 0.114\n",
      "Epoch: 328 | avg loss: 0.114\n",
      "Epoch: 329 | avg loss: 0.114\n",
      "Epoch: 330 | avg loss: 0.114\n",
      "Epoch: 331 | avg loss: 0.114\n",
      "Epoch: 332 | avg loss: 0.114\n",
      "Epoch: 333 | avg loss: 0.113\n",
      "Epoch: 334 | avg loss: 0.113\n",
      "Epoch: 335 | avg loss: 0.114\n",
      "Epoch: 336 | avg loss: 0.113\n",
      "Epoch: 337 | avg loss: 0.112\n",
      "Epoch: 338 | avg loss: 0.112\n",
      "Epoch: 339 | avg loss: 0.111\n",
      "Epoch: 340 | avg loss: 0.112\n",
      "Epoch: 341 | avg loss: 0.110\n",
      "Epoch: 342 | avg loss: 0.110\n",
      "Epoch: 343 | avg loss: 0.110\n",
      "Epoch: 344 | avg loss: 0.109\n",
      "Epoch: 345 | avg loss: 0.109\n",
      "Epoch: 346 | avg loss: 0.109\n",
      "Epoch: 347 | avg loss: 0.108\n",
      "Epoch: 348 | avg loss: 0.107\n",
      "Epoch: 349 | avg loss: 0.107\n",
      "Epoch: 350 | avg loss: 0.107\n",
      "Epoch: 351 | avg loss: 0.106\n",
      "Epoch: 352 | avg loss: 0.105\n",
      "Epoch: 353 | avg loss: 0.105\n",
      "Epoch: 354 | avg loss: 0.104\n",
      "Epoch: 355 | avg loss: 0.104\n",
      "Epoch: 356 | avg loss: 0.103\n",
      "Epoch: 357 | avg loss: 0.103\n",
      "Epoch: 358 | avg loss: 0.103\n",
      "Epoch: 359 | avg loss: 0.103\n",
      "Epoch: 360 | avg loss: 0.102\n",
      "Epoch: 361 | avg loss: 0.103\n",
      "Epoch: 362 | avg loss: 0.102\n",
      "Epoch: 363 | avg loss: 0.102\n",
      "Epoch: 364 | avg loss: 0.103\n",
      "Epoch: 365 | avg loss: 0.102\n",
      "Epoch: 366 | avg loss: 0.102\n",
      "Epoch: 367 | avg loss: 0.102\n",
      "Epoch: 368 | avg loss: 0.101\n",
      "Epoch: 369 | avg loss: 0.102\n",
      "Epoch: 370 | avg loss: 0.101\n",
      "Epoch: 371 | avg loss: 0.102\n",
      "Epoch: 372 | avg loss: 0.101\n",
      "Epoch: 373 | avg loss: 0.100\n",
      "Epoch: 374 | avg loss: 0.100\n",
      "Epoch: 375 | avg loss: 0.100\n",
      "Epoch: 376 | avg loss: 0.100\n",
      "Epoch: 377 | avg loss: 0.099\n",
      "Epoch: 378 | avg loss: 0.099\n",
      "Epoch: 379 | avg loss: 0.098\n",
      "Epoch: 380 | avg loss: 0.098\n",
      "Epoch: 381 | avg loss: 0.097\n",
      "Epoch: 382 | avg loss: 0.097\n",
      "Epoch: 383 | avg loss: 0.097\n",
      "Epoch: 384 | avg loss: 0.096\n",
      "Epoch: 385 | avg loss: 0.096\n",
      "Epoch: 386 | avg loss: 0.095\n",
      "Epoch: 387 | avg loss: 0.095\n",
      "Epoch: 388 | avg loss: 0.095\n",
      "Epoch: 389 | avg loss: 0.094\n",
      "Epoch: 390 | avg loss: 0.094\n",
      "Epoch: 391 | avg loss: 0.094\n",
      "Epoch: 392 | avg loss: 0.094\n",
      "Epoch: 393 | avg loss: 0.094\n",
      "Epoch: 394 | avg loss: 0.093\n",
      "Epoch: 395 | avg loss: 0.094\n",
      "Epoch: 396 | avg loss: 0.094\n",
      "Epoch: 397 | avg loss: 0.094\n",
      "Epoch: 398 | avg loss: 0.093\n",
      "Epoch: 399 | avg loss: 0.095\n",
      "Epoch: 400 | avg loss: 0.094\n",
      "Epoch: 401 | avg loss: 0.094\n",
      "Epoch: 402 | avg loss: 0.093\n",
      "Epoch: 403 | avg loss: 0.092\n",
      "Epoch: 404 | avg loss: 0.092\n",
      "Epoch: 405 | avg loss: 0.093\n",
      "Epoch: 406 | avg loss: 0.092\n",
      "Epoch: 407 | avg loss: 0.092\n",
      "Epoch: 408 | avg loss: 0.092\n",
      "Epoch: 409 | avg loss: 0.092\n",
      "Epoch: 410 | avg loss: 0.091\n",
      "Epoch: 411 | avg loss: 0.091\n",
      "Epoch: 412 | avg loss: 0.090\n",
      "Epoch: 413 | avg loss: 0.090\n",
      "Epoch: 414 | avg loss: 0.090\n",
      "Epoch: 415 | avg loss: 0.090\n",
      "Epoch: 416 | avg loss: 0.089\n",
      "Epoch: 417 | avg loss: 0.089\n",
      "Epoch: 418 | avg loss: 0.089\n",
      "Epoch: 419 | avg loss: 0.088\n",
      "Epoch: 420 | avg loss: 0.088\n",
      "Epoch: 421 | avg loss: 0.087\n",
      "Epoch: 422 | avg loss: 0.087\n",
      "Epoch: 423 | avg loss: 0.087\n",
      "Epoch: 424 | avg loss: 0.087\n",
      "Epoch: 425 | avg loss: 0.087\n",
      "Epoch: 426 | avg loss: 0.087\n",
      "Epoch: 427 | avg loss: 0.087\n",
      "Epoch: 428 | avg loss: 0.088\n",
      "Epoch: 429 | avg loss: 0.088\n",
      "Epoch: 430 | avg loss: 0.089\n",
      "Epoch: 431 | avg loss: 0.089\n",
      "Epoch: 432 | avg loss: 0.089\n",
      "Epoch: 433 | avg loss: 0.089\n",
      "Epoch: 434 | avg loss: 0.089\n",
      "Epoch: 435 | avg loss: 0.088\n",
      "Epoch: 436 | avg loss: 0.086\n",
      "Epoch: 437 | avg loss: 0.086\n",
      "Epoch: 438 | avg loss: 0.086\n",
      "Epoch: 439 | avg loss: 0.086\n",
      "Epoch: 440 | avg loss: 0.086\n",
      "Epoch: 441 | avg loss: 0.086\n",
      "Epoch: 442 | avg loss: 0.085\n",
      "Epoch: 443 | avg loss: 0.085\n",
      "Epoch: 444 | avg loss: 0.085\n",
      "Epoch: 445 | avg loss: 0.085\n",
      "Epoch: 446 | avg loss: 0.084\n",
      "Epoch: 447 | avg loss: 0.084\n",
      "Epoch: 448 | avg loss: 0.084\n",
      "Epoch: 449 | avg loss: 0.083\n",
      "Epoch: 450 | avg loss: 0.083\n",
      "Epoch: 451 | avg loss: 0.083\n",
      "Epoch: 452 | avg loss: 0.083\n",
      "Epoch: 453 | avg loss: 0.083\n",
      "Epoch: 454 | avg loss: 0.082\n",
      "Epoch: 455 | avg loss: 0.082\n",
      "Epoch: 456 | avg loss: 0.082\n",
      "Epoch: 457 | avg loss: 0.082\n",
      "Epoch: 458 | avg loss: 0.082\n",
      "Epoch: 459 | avg loss: 0.082\n",
      "Epoch: 460 | avg loss: 0.081\n",
      "Epoch: 461 | avg loss: 0.082\n",
      "Epoch: 462 | avg loss: 0.082\n",
      "Epoch: 463 | avg loss: 0.081\n",
      "Epoch: 464 | avg loss: 0.081\n",
      "Epoch: 465 | avg loss: 0.082\n",
      "Epoch: 466 | avg loss: 0.082\n",
      "Epoch: 467 | avg loss: 0.082\n",
      "Epoch: 468 | avg loss: 0.082\n",
      "Epoch: 469 | avg loss: 0.081\n",
      "Epoch: 470 | avg loss: 0.081\n",
      "Epoch: 471 | avg loss: 0.081\n",
      "Epoch: 472 | avg loss: 0.081\n",
      "Epoch: 473 | avg loss: 0.080\n",
      "Epoch: 474 | avg loss: 0.081\n",
      "Epoch: 475 | avg loss: 0.081\n",
      "Epoch: 476 | avg loss: 0.080\n",
      "Epoch: 477 | avg loss: 0.080\n",
      "Epoch: 478 | avg loss: 0.080\n",
      "Epoch: 479 | avg loss: 0.079\n",
      "Epoch: 480 | avg loss: 0.079\n",
      "Epoch: 481 | avg loss: 0.079\n",
      "Epoch: 482 | avg loss: 0.079\n",
      "Epoch: 483 | avg loss: 0.079\n",
      "Epoch: 484 | avg loss: 0.078\n",
      "Epoch: 485 | avg loss: 0.078\n",
      "Epoch: 486 | avg loss: 0.078\n",
      "Epoch: 487 | avg loss: 0.078\n",
      "Epoch: 488 | avg loss: 0.078\n",
      "Epoch: 489 | avg loss: 0.077\n",
      "Epoch: 490 | avg loss: 0.077\n",
      "Epoch: 491 | avg loss: 0.077\n",
      "Epoch: 492 | avg loss: 0.077\n",
      "Epoch: 493 | avg loss: 0.078\n",
      "Epoch: 494 | avg loss: 0.077\n",
      "Epoch: 495 | avg loss: 0.078\n",
      "Epoch: 496 | avg loss: 0.078\n",
      "Epoch: 497 | avg loss: 0.078\n",
      "Epoch: 498 | avg loss: 0.077\n",
      "Epoch: 499 | avg loss: 0.077\n"
     ]
    }
   ],
   "source": [
    "train_loop(500, Train_Loader)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
